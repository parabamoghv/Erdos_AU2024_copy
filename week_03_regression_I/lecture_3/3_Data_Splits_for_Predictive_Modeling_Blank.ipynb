{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d049c4b5",
   "metadata": {},
   "source": [
    "# Data Splits for Predictive Modeling\n",
    "\n",
    "When trying to build a predictive model we will have to split up our data set. In this notebook we touch on the reasons why and the standard procedures for doing so.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss the rationale for splitting our data set,\n",
    "- Introduce train test splits,\n",
    "- Define a validation set,\n",
    "- Review cross-validation and\n",
    "- Touch on scenarios in which you may prefer validation sets to cross-validation and versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062831f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will now start importing a common set\n",
    "## of items at the onset of most notebooks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a0e44",
   "metadata": {},
   "source": [
    "## Rationale for data splits\n",
    "\n",
    "### Goal of predictive modeling\n",
    "\n",
    "Imagine we are solving a predictive modeling problem, as we soon will. Once we have identified the problem we go out and randomly collect some data, $(X,\\vec{y})$, say $n$ observations of $m$ features and $n$ corresponding outputs. \n",
    "\n",
    "Our goal is to use these data to identify a model with the lowest <i>generalization error</i>. Generalization error is defined to be the error of the model on a new randomly collected set $(X^*, \\vec{y}^*)$. If we fix this new data set, then the fact that the data we collected originally, $(X,y)$, was randomly collected makes the generalization error of any particular model a random variable. Let's call this variable $G$.\n",
    "\n",
    "To determine the \"best\" model from a set of candidate models we would want to choose the one such that the corresponding $G$ is smallest. In other words for each candidate model we want to know something about the corresponding $G$ and its distribution.\n",
    "\n",
    "In an ideal world we would simply collect many sets, $(X,\\vec{y})$, fit our models on each of them and compare the resulting distributions. However, it may not be practical, possible or ethical to continually collect data for model selection purposes. In practice, thus, we are often constrained to a single data set for model fitting and comparisons.\n",
    "\n",
    "### Data splits\n",
    "\n",
    "In order to estimate $G$ we typically split our data (often more than once) so that we can use one part of the split to fit or estimate the model and the other part to estimate $G$.\n",
    "\n",
    "These splits are typically random because we want to be able to assume that the data used to fit the model follows the same distribution as the data used to measure generalization performance.\n",
    "\n",
    "We will now cover three splitting steps/strategies employed in data science and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032dcf34",
   "metadata": {},
   "source": [
    "## Train test splits\n",
    "\n",
    "The first split we will touch on is the first split you would do in a new data science project, the <i>train test split</i>.\n",
    "\n",
    "The purpose of the train test split is to create two data sets:\n",
    "1. <b>The training set</b> - This subset is used to fit models and compare model candidates. This data set is usually split further.\n",
    "2. <b>The testing set</b> - This subset is used as a final check on your selected model prior to putting your model into its desired final state.\n",
    "\n",
    "The training set usually contains the majority of the original data. Common train test split percentage divisions are $80\\% - 20\\%$ or $75\\% - 25\\%$, but it may sometimes be appropriate to use different split sizes. Train test splits are done randomly, with the form of randomness dependent upon your project.\n",
    "\n",
    "Here is an illustration of a train test split:\n",
    "\n",
    "<img src=\"lecture_3_assets/train_test.png\" width=\"40%\"></img>\n",
    "\n",
    "\n",
    "#### A potential point of confusion\n",
    "\n",
    "Perhaps confusingly, the test set is not directly used to compare models, model comparison is typically done using a subset(s) of the training set as we will soon see. The main purpose of the test is to serve as a final check on your chosen model. This final check is important! Checking model performance on the test set let's you look for coding/modeling errors as well as <i>overfitting</i> on the training set (we will talk about this more explicitly soon).\n",
    "\n",
    "### Performing train test splits in `sklearn`\n",
    "\n",
    "While you can use the `random` or `numpy.random` packages to perform the train test split by hand, the `sklearn` package has a useful `train_test_split` function that will perform the train test split. Here is a link to the documentation, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we will make a data set.  This is the same data set we used for the SLR notebook.\n",
    "np.random.seed(321)\n",
    "X = np.random.random(100)\n",
    "y = 1 + 2*X + 0.5*np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7d97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=12)\n",
    "plt.ylabel(\"$y$\", fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d47a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we make the split\n",
    "## train_test_split returns 4 outputs: X_train, X_test, y_train and y_test\n",
    "##\n",
    "## First you input the X and y for your data\n",
    "##\n",
    "## then set the shuffle argument to True, this randomly shuffles the\n",
    "## data before it is split\n",
    "##\n",
    "## The random_state ensures that the random split is the same each time\n",
    "## someone runs the code chunk, it can be any strictly positive integer\n",
    "##\n",
    "## You can specify the size of the test set with test_size,\n",
    "## here I want 20% of the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the data lengths to see that they match\n",
    "## what we'd expect\n",
    "print(\"The shape of X_train is\",X_train.shape)\n",
    "print(\"The shape of X_test is\",X_test.shape)\n",
    "print(\"The length of y_train is\",len(y_train))\n",
    "print(\"The length of y_test is\",len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.scatter(X_train, y_train, label=\"train\")\n",
    "plt.scatter(X_test, y_test, label=\"test\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=12)\n",
    "plt.ylabel(\"$y$\", fontsize=12)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47dfeb",
   "metadata": {},
   "source": [
    "## Two split types for model comparison and selection\n",
    "\n",
    "We will now cover two data splits you can make from the training set for model comparison purposes. Which you choose depends upon the project you are working on, but we will give some reasons to choose one over the other below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56435013",
   "metadata": {},
   "source": [
    "### Validation sets\n",
    "\n",
    "A <i>validation set</i> is a subset of the training data (the result of the train test split defined above) used solely for the purpose of comparing candidate models. This split is typically also performed randomly. Further, the validation set should be a small subset, common sizes range from $10\\%-25\\%$ of the training set depending on the training set size. An illustration of this concept is given below:\n",
    "\n",
    "<img src=\"lecture_3_assets/validation_set.png\" width=\"45%\"></img>\n",
    "\n",
    "The best model in this setting would be the one that has the best performance metric on the validation set.\n",
    "\n",
    "#### In practice\n",
    "\n",
    "In practice we can once again use `sklearn`'s `train_test_split` function to make the validation split. Note that it is good practice to not overwrite the original `X_train` or `y_train` sets when making the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we make a validation set with 15% of the \n",
    "## training data in the validation set\n",
    "X_train_train, X_val, y_train_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                                 shuffle = True,\n",
    "                                                                 random_state = 321,\n",
    "                                                                 test_size=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419372cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(\"15% of\",80,\"is\",.15*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b78400",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train_train\", X_train_train.shape)\n",
    "print(\"Shape of X_val\", X_val.shape)\n",
    "print(\"Length of y_train_train\", len(y_train_train))\n",
    "print(\"Length of y_val\", len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the split visually.\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.scatter(X_train_train, y_train_train, label=\"train_train\")\n",
    "plt.scatter(X_test, y_test, label=\"test\")\n",
    "plt.scatter(X_val, y_val, label=\"val\")\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x$\", fontsize=12)\n",
    "plt.ylabel(\"$y$\", fontsize=12)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346d703",
   "metadata": {},
   "source": [
    "### $k$-Fold cross-validation\n",
    "\n",
    "The validation set approach in essence gives us a <i>point estimate</i> of $G$. An issue with this approach for model selection is that point estimates are not always reflective of overall or even average model performance. What would be really nice is knowing something about the distribution from which $G$ is drawn. While this is difficult, we can leverage a well known rule from probability theory called the <i>law of large numbers</i>.\n",
    "\n",
    "#### The law of large numbers\n",
    "\n",
    "As a quick review let us remind ourselves what the law of large numbers says.\n",
    "\n",
    "Let $V_1, V_2, \\dots, V_n$ denote a sequence of independent identically distributed random variables with true mean $\\mu$. Let $\\overline{V} = \\frac{1}{n} \\left(V_1 + V_2 + \\dots + V_n \\right)$. The law of large numbers says that $\\lim_{n\\rightarrow\\infty} \\overline{V} = \\mu$\n",
    "\n",
    "What this says is that the arithmetic mean of a set of random draws will be \"close\" to the expected value of the distribution given enough draws.\n",
    "\n",
    "#### Leveraging the law of large numbers\n",
    "\n",
    "We can use this probability rule to our advantage to estimate the average (or expected) generalization error. If we can somehow generate a sequence of observations of this error, say $G_1, G_2, \\dots, G_n$, then we know that $\\overline{G} \\approx E(G)$. How do we generate such a sequence? Enter $k$-fold cross-validation.\n",
    "\n",
    "After conducting your train test split, you will randomly break your training set into $k$ equally sized (or roughly equal depending on how the division works out) chunks. You then generate \"observations\" of $G$ by cycling through each of the $k$ chunks. For each chunk you train your model on the $k-1$ other chunks and then calculate the error of that model on the chunk you held out. At the end you will have $k$ observations of $G$. Calculating the arithmetic mean of $G_1, \\dots, G_k$ gives you an estimate of $E(G)$.\n",
    "\n",
    "If those words were confusing let's look at some pictures instead.\n",
    "\n",
    "\n",
    "<img src=\"lecture_3_assets/cv1.png\" width=\"60%\"></img>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"lecture_3_assets/cv2.png\" width=\"60%\"></img>\n",
    "\n",
    "Common values for $k$ are $5$ and $10$.\n",
    "\n",
    "#### Implementing $k$-fold cross-validation in `sklearn`.\n",
    "\n",
    "You can implement cross-validation using `sklearn`'s `KFold` object. Documentation for this method can be found here <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7494a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe87b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a KFold object\n",
    "## n_splits controls the value of k\n",
    "## shuffle=True, randomly shuffles the data prior to splitting\n",
    "## random_state is the same as for train_test_split\n",
    "kfold = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## demonstrate.split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4cbe56",
   "metadata": {},
   "source": [
    "Side note on generators:  Notice that kfold.split returns a generator object.  If you are not familiar with them, you can think of this as being similar to a list except that instead of storing all of the elements in memory it stores the current element and a rule for getting the next element.\n",
    "\n",
    "I think that kfold is implemented this way to deal with memory issues if you use a large number of splits.  For instance, if a Leave Out One (LOO) split was implemented as a list on a dataset of size $10000$ the size of the list would be $10000*9999$.  If you use a generator instead then at each stage you only need to keep a list of size $10000$ in memory and also remember which element you should leave out next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1adf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use for loop to demonstrate .split\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    print(\"Train index:\", train_index)\n",
    "    print(\"Test index:\", test_index)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798875d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=5)\n",
    "klist = list(kfold.split(X_train, y_train))\n",
    "\n",
    "i=0\n",
    "for ax in axs:\n",
    "    ax.scatter(X_train[klist[i][0]],y_train[klist[i][0]]) \n",
    "    ax.scatter(X_train[klist[i][1]],y_train[klist[i][1]]) \n",
    "    ax.set_title(f'split {i}')\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f77bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit a different linear regression object for each split\n",
    "mses = np.zeros(5)\n",
    "i=0\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    ## get the kfold training data\n",
    "    X_train_train = \n",
    "    y_train_train = \n",
    "    \n",
    "    ## get the holdout data\n",
    "    X_holdout = \n",
    "    y_holdout = \n",
    "    \n",
    "    #store mse in the array\n",
    "    slr.fit()\n",
    "    mses[i] = mean_squared_error()\n",
    "    i+=1\n",
    "mses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5efd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the different model on each split\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5)\n",
    "klist = list(kfold.split(X_train, y_train))\n",
    "\n",
    "i=0\n",
    "for ax in axs:\n",
    "    ax.scatter(X_train[klist[i][0]],y_train[klist[i][0]]) \n",
    "    ax.scatter(X_train[klist[i][1]],y_train[klist[i][1]]) \n",
    "    ax.set_title(f'split {i}')\n",
    "\n",
    "    slr.fit(X_train[klist[i][0]].reshape(-1,1),y_train[klist[i][0]])\n",
    "    ax.plot(np.linspace(0,1,100), slr.predict(np.linspace(0,1,100).reshape(-1,1)), color = 'black')\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0d1ab",
   "metadata": {},
   "source": [
    "### Validation set or cross-validation\n",
    "\n",
    "Cross-validation, when feasible, is preferred to a single validation set. In general it is better to have a collection of estimates than just a single point estimate.\n",
    "\n",
    "However, it is not always feasible to perform cross-validation. Two limiting factors to consider are:\n",
    "1. Data set size and\n",
    "2. Model training time.\n",
    "\n",
    "In the case of 1., if you have too few observations cross-validation is not possible. This is because splitting your dataset into too many different sets can lead to deficiencies in both model fitting and estimation of $G$.\n",
    "\n",
    "Regarding 2. models that take prohibitively long to train limit the usefulness of cross-validation. $k$-fold cross-validation requires you to train the model $k$ distinct times.\n",
    "\n",
    "In either of those cases a validation set is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e870c74",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Updated by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61681cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
