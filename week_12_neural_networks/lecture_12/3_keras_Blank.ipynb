{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `keras`\n",
    "\n",
    "Let's now introduce a python package called `keras` which will provide us with greater versatility in building neural networks than `sklearn`.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will\n",
    "- Introduce `keras`:\n",
    "    - Discuss how to install it,\n",
    "- Review its syntax and\n",
    "- Demonstrate how to build feed forward networks in `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward we will be building our neural networks with `keras`.\n",
    "\n",
    "From their documentation:\n",
    "\n",
    "<blockquote>\n",
    "Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.\n",
    "</blockquote>\n",
    "\n",
    "In this notebook we will lay out how to build feed forward multi-layer networks.\n",
    "\n",
    "For reference, this material is being built by looking at the `keras` documentation <a href=\"https://keras.io/about/\">https://keras.io/about/</a> and the book, <a href=\"https://github.com/letspython3x/Books/blob/master/Deep%20Learning%20with%20Python.pdf\">Deep Learning with Python</a>. In particular this notebook's content comes from chapters 3 and 4 of that text.\n",
    "\n",
    "## Installation\n",
    "\n",
    "You may not have `keras` installed on your computer at this point. Try and run the following code chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# if the above did not work \n",
    "# uncomment and then try the below\n",
    "# from tensorflow import keras\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this ran for you, then you already have `keras` installed. For reference, the version of `keras` that I was running while writing this notebook was `2.11.0`.\n",
    "\n",
    "If you received an error while running that code chunk you likely need to install `keras` to move forward.\n",
    "\n",
    "##### Using `pip`\n",
    "\n",
    "If you use `pip` to install packages try running:\n",
    "\n",
    "`pip install keras`\n",
    "\n",
    "in your command line or terminal to install `keras`.\n",
    "\n",
    "##### Using `conda`\n",
    "\n",
    "If you use `conda` try what is recommended at this link, <a href=\"https://anaconda.org/conda-forge/keras\">https://anaconda.org/conda-forge/keras</a>.\n",
    "\n",
    "##### Apple M1 chip computers\n",
    "\n",
    "If you have an Apple computer with an M1 chip you may need extra help. Earlier in 2022 the `keras` package installation instructions did not yet play nicely with such hardware. Try performing a web search for relevant instructions.\n",
    "\n",
    "If you are unsure if you have an M1 chip, check with these instructions <a href=\"https://www.howtogeek.com/706226/how-to-check-if-your-mac-is-using-an-intel-or-apple-silicon-processor/\">https://www.howtogeek.com/706226/how-to-check-if-your-mac-is-using-an-intel-or-apple-silicon-processor/</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a neural net with `keras`\n",
    "\n",
    "### Classifying MNIST\n",
    "\n",
    "We will mimic our `sklearn` networks from the last notebook and build an MNIST classifier, using the `keras` version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This imports datasets stored in keras\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we load the data\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"og shape of X_train\", np.shape(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape the data to be a single column\n",
    "\n",
    "We will now reshape the data so that it has $60{,}000$ observations ($10{,}000$ for the test set) of $28\\times28$ pixels. This will give us a $60000 \\times 784$ $2$-D `numpy` array ($10000 \\times 784$ for the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,28*28)\n",
    "X_test = X_test.reshape(-1,28*28)\n",
    "\n",
    "print(\"The new shape of X_train is\", np.shape(X_train))\n",
    "print(\"The new shape of X_test is\", np.shape(X_test))\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the networks\n",
    "\n",
    "We start by importing all of the necessary pieces. We will explain each piece as we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import things\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# for earlier versions of keras run this instead\n",
    "# from keras.utils import to_categorical\n",
    "# instead of running\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# or look up the documentation for your version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we learned as feed forward networks in the last notebook are also called <i>dense</i> networks because they are fully connected graphs.\n",
    "\n",
    "We will now walk through the process of making a dense neural networks using `keras`.\n",
    "\n",
    "##### 1. Make an empty `model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we first make an empty model\n",
    "## Sequential means we'll make a group\n",
    "## of a linear stack of layers\n",
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Add the layers to the `model`\n",
    "\n",
    "We will build the following neural network architecture in this step.\n",
    "\n",
    "<img src=\"lecture_12_assets/mnist_net.png\" width=\"70%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ONLY RUN THIS ONCE! ##########\n",
    "\n",
    "## you add a layer with .add()\n",
    "## A Dense layer means a fully connected feedforward layer\n",
    "## the 16 means the layer is 16 nodes tall\n",
    "## activation='relu' means the layer uses a relu activation function\n",
    "## the first layer needs to be told the shape of the input data\n",
    "model\n",
    "\n",
    "# we then add a second layer that is 16 nodes tall\n",
    "# and uses the relu activation function\n",
    "# note we don't need the input shape here, \n",
    "# it is inferred from the first layer's output\n",
    "model\n",
    "\n",
    "\n",
    "# finally we add the output layer\n",
    "# this will have a single node, representing a probability\n",
    "# that the observation has positive sentiment\n",
    "# this is why the activation is a softmax\n",
    "# for more information on the softmax see the Practice Problems\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Compile the model with an optimizer,  loss and metric\n",
    "\n",
    "The optimizer we use is `rmsprop`, this is an algorithm implemented by `keras` to perform the backpropagation step in fitting the neural network.\n",
    "\n",
    "The loss we use is `categorical_crossentropy` which stems from information theory, this is the \"cost function\" we discussed in notebook 3. This is a common and popular choice for classification problems. See the HW notebook for an explanation of crossentropy.\n",
    "\n",
    "The metric we use is simply `accuracy`.\n",
    "\n",
    "Note that there are more options for all three of these choices and if you are interested in seeing more check out the `keras` documentation, it has an excellent search bar. There is also the possibility to use your own custom inputs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we compile the network like so\n",
    "## call .compile\n",
    "## set our optimizer, optimizer='rmsprop'\n",
    "## set our loss, loss='categorical_crossentropy'\n",
    "## set our desired metrics, metrics=['accuracy']\n",
    "model.compile(optimizer='rmsprop', \n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Fit the model on the training data\n",
    "\n",
    "We fit our model on the training data, we'll look at $100$ epochs that use batch gradient descent with a batch size of `512` observations per batch.\n",
    "\n",
    "We first create a validation set, this will be additional input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First make the validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_train,X_val,y_train_train,y_val = train_test_split(X_train, y_train,\n",
    "                                                          test_size=.2,\n",
    "                                                          shuffle=True,\n",
    "                                                          stratify=y_train,\n",
    "                                                          random_state=440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `to_categorical`\n",
    "\n",
    "In order for the desired output of our model to play nicely with the training and validation data we have to first turn each observation into a categorical output with the `keras` function `to_categorical`. Let's see what this does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print(to_categorical(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I now fit the model, and store the training history\n",
    "## I use 100 epochs and a batch_size of 512\n",
    "n_epochs = \n",
    "batch_size = \n",
    "history = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Examine epoch history loss and accuracy\n",
    "\n",
    "The data stored in `history` includes a dictionary with epoch information on both the training and validation losses and accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,n_epochs+1), history_dict['accuracy'], label = \"Training Accuracy\")\n",
    "plt.scatter(range(1,n_epochs+1), history_dict['val_accuracy'], marker='v', label = \"Validation Set Accuracy\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,n_epochs+1), history_dict['loss'], label = \"Training Loss\")\n",
    "plt.scatter(range(1,n_epochs+1), history_dict['val_loss'], marker='v', label = \"Validation Set Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss Function Value\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Tuning the model architecture\n",
    "\n",
    "Looking at data like this can allow us to choose a training period, i.e. the number of epochs, for a neural network as well as compare performance across two different networks. Let's try a comparison to a second network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Dense(32, activation='relu', input_shape=(28*28,)))\n",
    "model2.add(layers.Dense(32, activation='relu'))\n",
    "model2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer = 'rmsprop',\n",
    "                 loss = 'categorical_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train_train,\n",
    "                       to_categorical(y_train_train),\n",
    "                       epochs = n_epochs,\n",
    "                       batch_size = batch_size,\n",
    "                       validation_data = (X_val, to_categorical(y_val)))\n",
    "\n",
    "history_dict2 = history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,n_epochs+1), history_dict['val_accuracy'], label = \"Neural Net 1 Val. Accuracy\")\n",
    "plt.scatter(range(1,n_epochs+1), history_dict2['val_accuracy'], marker='v', label = \"Neural Net 2 Val. Accuracy\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,n_epochs+1), history_dict['val_loss'], label = \"Neural Net 1 Val. Loss\")\n",
    "plt.scatter(range(1,n_epochs+1), history_dict2['val_loss'], marker='v', label = \"Neural Net 2 Val. Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Selecting an architecture\n",
    "\n",
    "It looks like the 32 x 32 network edges out the 16 x 16. Let's find the epoch that resulted in the lowest validation loss and use that for our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The epoch that had the lowest model 2 validation loss was\",\n",
    "     range(1,n_epochs)[np.argmin(history_dict2['val_loss'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "\n",
    "model2.add(layers.Dense(32, activation='relu', input_shape=(28*28,)))\n",
    "model2.add(layers.Dense(32, activation='relu'))\n",
    "model2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer = 'rmsprop',\n",
    "                 loss = 'categorical_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "history2 = model2.fit(X_train,\n",
    "                       to_categorical(y_train),\n",
    "                       epochs = range(1,n_epochs+1)[np.argmin(history_dict2['val_loss'])],\n",
    "                       batch_size = 512,\n",
    "                       validation_data = (X_val, to_categorical(y_val)))\n",
    "\n",
    "history_dict2 = history2.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Predicting on the validation set\n",
    "\n",
    "Let's now use this model to predict on the validation set. This is quite similar to the procedure you would follow in the `sklearn` setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this produces a set of probabilities for each observation. For our prediction of an actual observation we can just choose the one with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model2.predict(X_val), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(100*accuracy_score(y_val, np.argmax(model2.predict(X_val), axis=1)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
