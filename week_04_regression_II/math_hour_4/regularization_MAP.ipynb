{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The supervised parametric machine learning framework we have been using can be summarized as follows:\n",
    "\n",
    "1.  We have some data $(\\vec{x}_i,\\vec{y}_i)$ for $i \\in \\{1,2,3,\\dots, n\\}$. where $\\vec{x}_i \\in \\mathbb{R}^p$ and $\\vec{y}_i \\in \\mathbb{R}^m$.\n",
    "2.  We presume that there is a true functional relationship $f: \\mathbb{R}^p \\to \\mathbb{R}^m$, but that this is corrupted by some error term:\n",
    "\n",
    "$$\n",
    "\\vec{y} = f(\\vec{x}) + \\epsilon \n",
    "$$\n",
    "\n",
    "3.  We cannot search over **all** functions for a candidate $\\hat{f}$, so we instead specify some collection of functions $f_\\theta$ which depend on some parameters $\\theta$.\n",
    "4.  We introduce a loss function $\\ell(\\theta)$ to measure how good a job $f_\\theta$ does on the training data.  $\\ell$ takes $\\theta$ as input and returns a non-negative real number as output.\n",
    "5.  We choose $\\hat{\\theta}$ so that $\\ell(\\hat{\\theta})$ is minimized or, if this is impossible, at least approximate a local minimum. \n",
    "\n",
    "In the case of least squares linear regression we use $f_{\\vec{\\theta}} (\\vec{x}) = \\vec{\\theta} \\cdot \\vec{x}$ as our class of functions and $\\ell(\\vec{\\theta}) = \\sum_1^n |y_i - \\vec{x}_i \\cdot \\vec{\\theta}|^2$ as our loss function.\n",
    "\n",
    "**Regularization** is modifying our loss function to penalize solutions which are \"extreme\" in some way.\n",
    "\n",
    "We already saw two common regularization methods in lecture this week: \"Ridge Regression\" which adds a term of the form $\\lambda |\\vec{\\theta}|^2$ to the loss and \"Lasso Regression\" which adds a term of the form $\\lambda |\\vec{\\theta}|$ to the loss.  Both methods \"penalize\" solutions with large parameters.\n",
    "\n",
    "In this notebook we will \n",
    "\n",
    "*  See how both methods can be viewed as maximum a posteriori probability estimates of the parameters given different priors.\n",
    "*  See how using linear regression with a ridge penalty is equivalent to having $p$ \"pseudo-observations\" of a particularly nice form.\n",
    "*  See how using linear regression with a ridge penalty is related to PCA linear regression.\n",
    "    * PCA linear regression \"throws away\" singular values less than a given cut-off value.\n",
    "    * Ridge regression reduces all singular values, but shrinks smaller singular values more.  So it can be seen as a \"smooth\" version of PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regularization (for linear regression) as MAP with Gaussian Priors\n",
    "\n",
    "The assumption of linear regression is that\n",
    "\n",
    "$$y = \\vec{x} \\cdot \\vec{\\theta} + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\sigma)$.\n",
    "\n",
    "To make the story (and algebra!) a bit less complicated, let's assume that $\\sigma = 1$.\n",
    "\n",
    "One way to formulate our desire for the parameters $\\vec{\\theta}$ to be small is to suppose that $\\vec{\\theta} \\sim \\mathcal{N}(0, \\frac{1}{\\lambda} I_p)$ (the multivariate Gaussian with with mean $\\vec{0}$ and variance matrix $\\frac{1}{\\lambda} I_p$).  Here $\\lambda$ is just some positive constant.  When we choose a larger $\\lambda$ we are expressing greater confidence in our guess that $\\vec{\\theta}$ is small.  \n",
    "\n",
    "The assumption of linear regression (with $\\sigma = 1$) is that\n",
    "\n",
    "$$y = \\vec{x} \\cdot \\vec{\\theta} + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, 1)$.\n",
    "\n",
    "Let's use the following notation for the pdf of $\\mathcal{N}(0, t)$:\n",
    "\n",
    "$$\\phi_t(x) = \\frac{1}{\\sqrt{2\\pi t}}\\exp\\left( - \\frac{x^2}{2 t^2} \\right)$$\n",
    "\n",
    "Then we have\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\vec{\\theta}, \\sigma | \\textrm{Data}) \n",
    "&\\propto P(\\textrm{Data} | \\vec{\\theta}) P(\\vec{\\theta}) \\\\\n",
    "&= \\left(\\prod_1^n \\phi_{1}(y_i - \\vec{x}_i \\cdot \\vec{\\theta}) \\right) \\phi_{\\frac{1}{\\lambda}}(|\\vec{\\theta}|)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As usual we take the negative log likelihood to get $\\ell$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell (\\vec{\\theta}) \n",
    "&= \\frac{n}{2}\\log(2\\pi) + \\left( \\frac{1}{2}\\sum_{i=1}^n \\left(y_i - \\vec{x} \\cdot \\vec{\\theta}\\right)^2 \\right) + \\frac{\\lambda}{2} |\\vec{\\theta}|^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we have \n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\mathop{\\arg \\min}\\limits_{\\vec{\\theta}}  \\left( \\sum_{i=1}^n \\left(y_i - \\vec{x}_i \\cdot \\vec{\\theta}\\right)^2 \\right)+ \\lambda |\\vec{\\theta}|^2\n",
    "$$\n",
    "\n",
    "This shows that ridge regression (for linear regression) is just MAP with a normal prior for our parameters!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regularization (for linear regression) as MAP with Laplacian priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make $\\theta_i \\sim \\operatorname{Laplace}(0, \\frac{2}{\\lambda})$ instead, where $\\operatorname{Laplace}$ is the [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution).  It's pdf is \n",
    "\n",
    "$$f(t) = \\lambda \\exp(-\\frac{1}{2} \\lambda |t|)$$\n",
    "\n",
    "\n",
    "The only thing that changes in our analysis above is that our \"extra term\" is \n",
    "\n",
    "$$\n",
    "-  \\sum_{j = 1}^{p} \\log\\left( \\lambda \\exp(-\\frac{1}{2} \\lambda \\theta_i) \\right) = -p\\log(\\lambda) + \\frac{\\lambda}{2}  \\sum_{j = 1}^{p} |\\theta_i|\n",
    "$$\n",
    "\n",
    "Remembering that our $\\lambda$ is fixed (so the term $-p\\log(\\lambda)$ doesn't impact our minimization problem ) we have\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\mathop{\\arg \\min}\\limits_{\\vec{\\theta}}  \\left( \\sum_{i=1}^n \\left(y_i - \\vec{x}_i \\cdot \\vec{\\theta}\\right)^2 \\right) + \\lambda \\sum_{j = 1}^{p} |\\vec{\\theta}_i|\n",
    "$$\n",
    "\n",
    "This shows that lasso regression (for linear regression) is just MAP with a Laplace prior for our parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression as pseudo-observations\n",
    "\n",
    "Elements of Statistical Learning exercise 3.12 gives another way to understand ridge regression.\n",
    "\n",
    "![Problem 3.12 of ESL](math_hour_4_assets/ESL-3.12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see why this works, and then discuss a bit about the intuition here.\n",
    "\n",
    "\n",
    "First we need to see how to find $\\hat{\\theta}_{\\textrm{ridge}}$ explicitly.\n",
    "\n",
    "$$\n",
    "\\ell_\\lambda (\\vec{\\theta}) = |\\vec{y} - X \\vec{\\theta}|^2 + \\lambda |\\vec{\\theta}|^2\n",
    "$$\n",
    "\n",
    "Taking gradients:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\ell_\\lambda \\big \\vert_{\\vec{\\theta}} \n",
    "&= -2X^\\top (\\vec{y} - X \\vec{\\theta}) + 2\\lambda \\vec{\\theta}\\\\\n",
    "&= 2(X^\\top X + \\lambda I) \\vec{\\theta} - 2X^\\top \\vec{y}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so $\\nabla \\ell_\\lambda \\big \\vert_{\\vec{\\theta}_{\\textrm{ridge}}} = 0$ has solution\n",
    "\n",
    "$$\n",
    "\\vec{\\theta}_{\\textrm{ridge}} = (X^\\top X + \\lambda I)^{-1} X^\\top \\vec{y}\n",
    "$$\n",
    "\n",
    "It is worth noting that while $X^\\top X$ is only invertible if $X$ has linearly independent columns, $X^\\top X + \\lambda I$ is a positive definite symmetric matrix, and is thus invertible without any assumptions on $X$.\n",
    "\n",
    "Let $X_{\\textrm{aug}}$ be the $(n+p) \\times p$ matrix  $\\begin{bmatrix} X \\\\ \\sqrt{\\lambda} I_p \\end{bmatrix}$, where I am using vertical juxtaposition to indicate how these are stacked.  Let $\\vec{y}_{\\textrm{aug}}$ be the $(n+p)$ vector $\\begin{bmatrix} \\vec{y} \\\\ \\vec{0}_p\\end{bmatrix}$.\n",
    "\n",
    "Then the ordinary least squares regression of  $\\vec{y}_{\\textrm{aug}}$ on $X_{\\textrm{aug}}$ gives us:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{\\theta}_{\\textrm{aug}} \n",
    "&=  (X_{\\textrm{aug}}^\\top X_{\\textrm{aug}})^{-1}X_{\\textrm{aug}}^\\top \\vec{y}_{\\textrm{aug}}\\\\\n",
    "&=  (\\begin{bmatrix} X^\\top &  \\sqrt{\\lambda} I_p \\end{bmatrix} \\begin{bmatrix} X \\\\ \\sqrt{\\lambda} I_p \\end{bmatrix})^{-1}\\begin{bmatrix} X^\\top &  \\sqrt{\\lambda} I_p \\end{bmatrix} \\begin{bmatrix} \\vec{y} \\\\ \\vec{0}_p\\end{bmatrix}\\\\\n",
    "&= (X^\\top X + \\lambda I_p)^{-1} X^\\top \\vec{y}\\\\\n",
    "&= \\vec{\\theta}_{\\textrm{ridge}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This gives a completely different reason for being interested in th ridge regression solution:\n",
    "\n",
    "If your design matrix $X$ had linearly dependent columns (or just \"nearly\" linearly dependent), then $X^\\top X$ would be non-invertible (or, if there is a \"near\" linear dependence it will have a very bad \"condition number\").  One way to address this issue would be to invent some fake data to make the columns linearly independent.  A very natural choice is to invent one observation (row) for each feature where only that feature is non-negative, and where the response variable is set to 0 for this observation.  This makes sure the columns are linearly independent!  These observations also clearly bias the estimate of the coefficients towards zero.\n",
    "\n",
    "Choosing to use the same constant $\\sqrt{\\lambda}$ for all of these \"psuedo-observations\" gives us ridge regression.\n",
    "\n",
    "It is interesting to think about how this approach could be tailored to particular situations:\n",
    "\n",
    "* If you know that the columns of $X$ are linearly independent aside from a subset of columns, you might be able to get away with just inventing \"fake data\" for these columns.\n",
    "* If you care about some features more than others you might use a non-constant diagonal matrix in place of the diagonal one.   This corresponds to minimizing the loss function $$\n",
    "* Both of these correspond to minimizing a loss function of the form $|\\vec{y} - X \\vec{\\theta}|^2 + |W \\vec{\\theta}|^2$ where $W$ is a diagonal matrix.  One could also generalize by having $W$ be an arbitrary matrix.  This would correspond to augmenting the design matrix by $W^\\top W$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SVD to understand the impact of $L_2$ regularization.\n",
    "\n",
    "We can use the SVD to gain a little insight into what ridge regression is doing.\n",
    "\n",
    "Let $X = U \\Sigma V^\\top$ with $V$ an orthogonal $n \\times n$ matrix, $\\Sigma$ a diagonal $n \\times n$ matrix, and $U$ an orthogonal $n \\times p$ matrix.  Note that this is a bit different from the factorizations we have used so far:  we are using all of the right singular vectors, but we are leaving out any left singular vectors beyond the $p^{th}$ one.  This is fine:  if you remember from the construction, those are all perpendicular to the image of $X$ and were chosen only to complete a basis of $\\mathbb{R}^n$.\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{\\theta}_{\\textrm{ridge}}\n",
    "&= (X^\\top X + \\lambda I_p)^{-1} X^\\top \\vec{y}\\\\\n",
    "&= ( V \\Sigma U^\\top U \\Sigma V^\\top + \\lambda I_p)^{-1} V\\Sigma U^\\top \\vec{y}\\\\\n",
    "&= ( V \\Sigma^2 V^\\top + \\lambda V V^\\top)^{-1} V \\Sigma U^\\top \\vec{y}\\\\\n",
    "&= ( V (\\Sigma^2 + \\lambda I_p) V^\\top)^{-1} V \\Sigma U^\\top \\vec{y}\\\\\n",
    "&=  V (\\Sigma^2 + \\lambda I_p)^{-1} V^\\top V \\Sigma U^\\top \\vec{y}\\\\\n",
    "&=  V (\\Sigma^2 + \\lambda I_p)^{-1} \\Sigma U^\\top \\vec{y}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Setting $\\lambda = 0$ we obtain the OLS solution\n",
    "\n",
    "$$\n",
    "\\vec{\\theta}_{\\textrm{ols}} = V \\Sigma^{-1} U^\\top \\vec{y}\n",
    "$$\n",
    "\n",
    "So the coefficients for the $\\vec{\\theta}_{\\textrm{ols}}$ with respect to the basis $V$ are $\\Sigma^{-1} U^\\top \\vec{y}$, while the coefficients for $\\vec{\\theta}_{\\textrm{ridge}}$ with respect to the same basis are $(\\Sigma^2 + \\lambda I_p)^{-1} \\Sigma U^\\top \\vec{y}$.  These are the same, except that the coefficient of $\\vec{v}_j$ has been scaled by $\\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}$. Notice that $\\vec{v}_j$ which have *larger* singular values are \"shrunk\" less than $\\vec{v}_j$ which have smaller singular values.  \n",
    "\n",
    "This should make some intuitive sense.  If the right singular vectors with larger singular values are carrying \"more of the signal\" about the data $X$, then when we are \"on an $L_2$ budget\" we will want to prioritize those.\n",
    "\n",
    "We can also apply $X$ to the ridge solution to see how to compare $\\hat{y}_{\\textrm{ridge}}$ vs. $\\hat{y}_{\\textrm{ols}}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_{\\textrm{ridge}} \n",
    "&= X\\vec{\\theta}_{\\textrm{ridge}}\\\\\n",
    "&=  U \\Sigma V^\\top V (\\Sigma^2 + \\lambda I_p)^{-1} \\Sigma U^\\top \\vec{y}\\\\\n",
    "&=  U \\Sigma^2 (\\Sigma^2 + \\lambda I_p)^{-1} U^\\top \\vec{y}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "vs.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{\\textrm{ols}} = U U^\\top \\vec{y}\n",
    "$$\n",
    "\n",
    "So again, you scale the $\\vec{u}_j$ coordinates of the OLS solution by $\\frac{\\sigma_j^2}{\\sigma_j^2 + \\alpha}$ to obtain the ridge regression coordinates.\n",
    "\n",
    "This gives yet another interpretation of why Ridge Regression is helpful:  it can be thought of as a kind of \"smooth version\" of PCA instead of just discarding small singular values we instead shrink smaller principle components more than large principle components."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
