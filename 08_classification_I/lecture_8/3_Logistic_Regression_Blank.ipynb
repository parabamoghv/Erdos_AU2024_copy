{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Learn the logistic regression algorithm,\n",
    "- Show how you can interpret logistic regression output,\n",
    "- Talk about classification cutoffs and\n",
    "- Think about predicting probabilities instead of hard classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explain using a toy example.  A standard apple tree which is only $1$ foot tall is unlikely to bear fruit this year.  However an apple tree which is $20$ feet tall is very likely to be of bearing age.  We might expect an increasing relationship between the height of the tree and the probability that the tree is capable of bearing fruit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_vs_bearing = pd.read_csv('lecture_8_assets/height_vs_bearing.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_vs_bearing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have measured the height of 100 apple trees, and recorded whether or not the tree is currently bearing apples.\n",
    "\n",
    "Here is a graph of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.scatter(height_vs_bearing['height'], height_vs_bearing['bearing'])\n",
    "\n",
    "plt.xlabel(\"Height\", fontsize=12)\n",
    "plt.ylabel(\"Bearing\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, when the tree is shorter than 9 feet tall it is very rare for it to bear apples.  When it is taller than 11.5 feet tall it is almost surely going to bear apples.  In between we have a murkier situation.  One thing we could do to try to get a better understanding of this relationship is try and see how the probability of bearing varies as a function of height.  We can estimate the probability $P(\\textrm{bearing} = 1 | x )$ by looking at the empirical probability of bearing in a small neighborhood of $x$, say $$P(\\textrm{bearing} = 1 | \\textrm{height} = x ) \\approx \\frac{\\textrm{\\# samples bearing}}{\\textrm{\\# samples in }[x-0.5,x+0.5]}$$  \n",
    "\n",
    "Let's take a look at that graph.  I have highlighted the point with $\\textrm{height} = 9.5$ green.  In the interval $[9,10]$ there are $20$ samples of which $5$ are bearing, for an estimated probability of $\\frac{5}{20} = 0.25$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Approximate probabilities](lecture_8_assets/approx-probs.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to model the probability as a funtion of height directly.  Our model might look something like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![approximate probabilities with logistic regression curve](lecture_8_assets/approx-probs-with-curve.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression we will model the probability $P(\\textrm{bearing} = 1| \\textrm{height} = x)$ using a function of the form $p(x) = \\sigma(mx+b)$ where $\\sigma$ is the *sigmoid function* defined by\n",
    "\n",
    "$$\n",
    "\\sigma(t) = \\frac{1}{1+e^{-t}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret this as follows\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&p(x) = \\frac{1}{1+e^{-(mx+b)}}\\\\\n",
    "&\\frac{1}{p(x)} = 1 + e^{-(mx+b)}\\\\\n",
    "&\\frac{1 - p(x)}{p(x)} = e^{-(mx+b)}\\\\\n",
    "&\\frac{p(x)}{1-p(x)} = e^{mx+b}\\\\\n",
    "&\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = mx+b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The quantity $\\frac{p(x)}{1-p(x)}$ is the [odds](https://en.wikipedia.org/wiki/Odds) in favor of bearing.\n",
    "\n",
    "So by modeling our probability using the parameteric family $p(x) = \\sigma(mx+b)$ we are making the assumption that the log-odds in favor of bearing is approximately linear in the height.  In other words, we are assuming that each additional foot of height increases the log-odds in favor of bearing by a fixed amount ($m$).  Phrased in terms of the odds, we are saying that each additional foot of height multiplies the odds in favor of bearing by $e^m$.\n",
    "\n",
    "This assumption may or may not be reasonable, just like fitting a linear regression may or may not be reasonable.  In this case our exploratory plot makes it look like a reasonable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing to multiple features\n",
    "\n",
    "We have $n$ samples which each have $p$ continuous variables $x_1, x_2, x_3, ..., x_p$ and one binary variable $y$ (which is either $0$ or $1$ for each sample).  We want to model the probability that $y = 1$ as a function of these variables using a model with parameters $\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3, ..., \\beta_p)$ where $\\beta \\in \\mathbb{R}^{p+1}$.  In other words, we want to model the probability as some function $p_\\beta: \\mathbb{R}^p \\to [0,1]$ which depends on these parameters.  \n",
    "\n",
    "The logistic regression model is that\n",
    "\n",
    "$$\n",
    "p(y = 1 | \\vec{x}) = \\sigma(\\vec{x} \\cdot \\vec{\\beta})\n",
    "$$\n",
    "\n",
    "here we are adopting the convention of augmenting $\\vec{x}$ with an initial $1$ to capture the intercept term.\n",
    "\n",
    "The interpretation of the fit parameters $\\hat{\\beta}$ is the same:  if $x_j$ increases by one unit, the log-odds that the $y=1$ increases by $\\hat{\\beta}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Binary Cross Entropy Loss Function\n",
    "\n",
    "We will increase the generality a bit and think about an appropriate loss function for **any** conditional probability with binary outcomes.\n",
    "\n",
    "$$P(y = 1 | \\vec{x} ; \\vec{\\beta}) = p_{\\vec{\\beta}}(\\vec{x})$$\n",
    "\n",
    "The question we have is how to select the parameters $\\beta$ so as to maximize the likelihood that the data in our sample was generated by the model. \n",
    "\n",
    "Fix the parameters $\\beta$.  For a single observation $(\\vec{x},y)$ the model predicts this observation will occur with a probability of\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "p_\\beta(\\vec{x}) \\textrm{ if $y = 1$}\\\\\n",
    "1 - p_\\beta(\\vec{x}) \\textrm{ if $y = 0$}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We can write this more compactly as the single expression \n",
    "\n",
    "$$\n",
    "y p_\\beta(\\vec{x}) + (1-y)(1 - p_\\beta(\\vec{x}))\n",
    "$$\n",
    "\n",
    "So our model predicts that the probability that our sample of $N$ observations occurs is \n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^{n} \\left(y_i p_\\beta(\\vec{x_i}) + (1-y_i)(1 - p_\\beta(\\vec{x_i}))\\right)\n",
    "$$\n",
    "\n",
    "Our goal is to find the value of $\\beta$ which maximizes this quantity.\n",
    "\n",
    "We are now going to take the logarithm of the likelihood function:\n",
    "\n",
    "$$\n",
    "\\log(L(\\beta)) = \\sum_{i = 1}^{n} y_i \\log (p_\\beta(\\vec{x_1}))  + (1 - y_i)\\log(1 - p_\\beta(\\vec{x_i}))\n",
    "$$\n",
    "\n",
    "Why did we take the logarithm?  Several reasons:\n",
    "\n",
    "* The logarithm is monotonically increasing, so the same $\\beta$ maximizes both $L$ and $\\log L$.\n",
    "* The values of $L$ are probably quite small.  We are multiplying together lots of numbers which are between $0$ and $1$.  The result will be a tiny positive number.  Using a logarithmic transformation is a nice way to put these numbers on a more reasonable scale.\n",
    "* We will eventually want to differentiate to find a maximum value.  The derivative of the likelihood uses the product rule, which would involve a product over all observations.  This is not very parallelizable.  The derivative of the log-likelihood only uses the sum rule, which results in a parallelizable derivative computation.\n",
    "\n",
    "The log-likelihood is a negative number which we are trying to maximize.  By convention we will instead minimize the negative log-likelihood:\n",
    "\n",
    "$$\n",
    " \\ell(\\beta) = - \\log(L(\\beta)) = - \\sum_{i = 1}^{n} y_i \\log (p_\\beta(\\vec{x_1}))  + (1 - y_i)\\log(1 - p_\\beta(\\vec{x_i}))\n",
    "$$\n",
    "\n",
    "where I am using $\\ell$ to stand for \"loss\".\n",
    "\n",
    "This expression is called the [binary cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy) between the model $p_\\beta$ and the empirical probability from our observations $(\\vec{x_i}, y_i)$.\n",
    "\n",
    "You can do the calculus (or check the math hour notebook for a derivation) to see that in the particular case of Logistic Regression, where we use $p_{\\vec{\\beta}}(\\vec{x}) = \\sigma(\\vec{x} \\cdot \\vec{\\beta})$, we have\n",
    "\n",
    "$$\n",
    "\\nabla \\ell \\big|_{\\vec{\\beta}} =  X^\\top (\\sigma(X\\vec{\\beta}) - \\vec{y})\n",
    "$$\n",
    "\n",
    "where, as usual, we are recording our observations in a design matrix $X$ which has been augmented by an initial column of ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize the loss function we need to find where the gradient vanishes.  Unfortunately, we cannot solve this analytically.  We will instead resort to numerical approximation.  \n",
    "\n",
    "We first show a custom implementation using gradient descent and then show you how to use the `sklearn` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got this numerically stable implementation from Tim Vieira's blog post here\n",
    "# https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "def sigmoid_func(x):\n",
    "    '''Numerically stable sigmoid function.'''\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        # if x is less than zero then z will be small, denom can't be\n",
    "        # zero because it's 1+z.\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)\n",
    "\n",
    "sigmoid = np.vectorize(sigmoid_func)\n",
    "\n",
    "class CustomLogisticRegression():\n",
    "    def __init__(self):\n",
    "        self.beta = None\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.1, tolerance=0.00001):\n",
    "        '''Fits beta using gradient descent'''\n",
    "        self.beta = np.random.randn(X.shape[1]+1)\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X]) # adding column of ones to X\n",
    "        Xt = X.transpose()\n",
    "        grad = (1/X.shape[0])*np.dot(Xt, sigmoid(np.dot(X, self.beta) - y)) # formula for gradient of the binary cross entropy\n",
    "        while np.linalg.norm(grad)>tolerance:\n",
    "            self.beta -= learning_rate*grad # update beta\n",
    "            grad = (1/X.shape[0])*np.dot(Xt, sigmoid(np.dot(X, self.beta)) - y) # update gradient\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "        return sigmoid(np.dot(X, self.beta))\n",
    "        \n",
    "    def predict(self, X, threshold = 0.5):\n",
    "        return 1*(self.predict_proba(X) > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 10 seconds on my computer\n",
    "X = height_vs_bearing[['height']].values\n",
    "y = height_vs_bearing['bearing'].values\n",
    "my_log_reg = CustomLogisticRegression()\n",
    "my_log_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn for Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to our gradient descent function, scikit-learn fits the model almost instantly!  They are using more sophisticated numerical analysis.  \n",
    "\n",
    "See [the Broyden–Fletcher–Goldfarb–Shanno algorithm](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) for more details.\n",
    "\n",
    "Also note that the `sklearn` includes l2 regularization by default.  If we want it to be unregularized we have to pass the option `penalty = None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import model\n",
    "\n",
    "## Instantiate model\n",
    "log_reg = \n",
    "\n",
    "## fit the model\n",
    "\n",
    "\n",
    "print(f'Our gradient descent function obtains the fit m = {my_log_reg.beta[1]} and b = {my_log_reg.beta[0]}')\n",
    "print(f'sklearn obtains the fit m = {log_reg.coef_[0][0]} and b = { log_reg.intercept_[0]}')\n",
    "print('These are pretty close!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate .coef_, .intercept_, .predict_proba, and .predict\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the fit generated by sklearn looks.  You can check that the fit we obtained using gradient descent is indistinguishable to the human eye by uncommenting a line in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "plt.plot(np.linspace(7,13), log_reg.predict_proba(np.linspace(7,13).reshape(-1,1))[:,1], color = 'orange')\n",
    "#plt.plot(np.linspace(7,13), my_log_reg.predict_proba(np.linspace(7,13).reshape(-1,1)), c = 'k')\n",
    "\n",
    "plt.xlabel(\"Height (ft)\", fontsize=12)\n",
    "plt.ylabel(\"Bearing\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From probabilities to classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate classifications from these predicted probabilities by choosing a *threshold*. \n",
    "\n",
    "For instance if $p(\\vec{x})$ exceeds the threshold of $0.5$ we classify the instance as $1$, otherwise we say it is a $0$.  This is the most common threshold and is the default threshold used by `sklearn` to convert `.predict_proba` values into `.predict` classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the cutoff\n",
    "cutoff = .7\n",
    "\n",
    "## store the predicted probabilities\n",
    "y_prob = \n",
    "\n",
    "## assign the value based on the cutoff\n",
    "y_pred = \n",
    "\n",
    "## print the accuracy\n",
    "## input the accuracy after \"is\",\n",
    "print(\"The training accuracy for a cutoff of\",cutoff,\n",
    "      \"is\", np.sum(y_pred == y)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now plot how the accuracy changes with the cutoff\n",
    "cutoffs = np.arange(0,1.01,.01)\n",
    "accs = []\n",
    "\n",
    "for cutoff in cutoffs:\n",
    "    y_pred = 1*(y_prob >= cutoff)\n",
    "    accs.append(np.sum(y_pred == y)/len(y))\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(cutoffs,accs)\n",
    "\n",
    "plt.xlabel(\"Cutoff\",fontsize=12)\n",
    "plt.ylabel(\"Training Accuracy\",fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should think carefully about how you select your decision boundary for classification.\n",
    "\n",
    "For example consider the following two extremely different scenarios:\n",
    "\n",
    "1. You use your classifier to decide which of two advertisements, which both market the same product, to show to a user on your website.\n",
    "2. You use your classifier to decide whether or not to give a patient an additional diagnostic test for cancer.\n",
    "\n",
    "In case 1 it might be reasonable to use a decision boundary of $0.5$.\n",
    "\n",
    "In case 2 it would be an extremely *bad* idea to only give a follow up test if you predict cancer with greater than 50% probability.  You probably want to set the threshold much lower, and your selection of the boundary should carefully weigh the costs and benefits of false positives, false negatives, true positives, and true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear decision boundaries\n",
    "\n",
    "If we have $p$ features then our fit logistic regression model will be of the form\n",
    "\n",
    "$$\n",
    "P(y = 1 | \\vec{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta \\cdot \\vec{x})}}\n",
    "$$\n",
    "\n",
    "where I have temporarily changed conventions about $\\vec{x}$ including an initial $1$.\n",
    "\n",
    "If we set a decision boundary of $c$, then this boundary is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{1 + e^{-(\\beta_0 + \\beta \\cdot \\vec{x})}} &= c\\\\\n",
    "1 + e^{-\\beta_0 + \\beta \\cdot \\vec{x}} &= c^{-1}\\\\\n",
    "e^{-\\beta_0 + \\beta \\cdot \\vec{x}} &= \\frac{1-c}{c}\\\\\n",
    "\\beta_0 + \\beta \\cdot \\vec{x} &= \\log(\\frac{c}{1-c})\\\\\n",
    "\\beta \\cdot \\vec{x} &= -\\beta_0 + \\log(\\frac{c}{1-c})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This is a hyperplane in $\\mathbb{R}^p$.  Note that adjusting $c$ gives you a family of parallel hyperplanes.\n",
    "\n",
    "To illustrate this, I generate two classes with multivariate normally distributed features (but different means) and plot the decision boundary using $c = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.random.multivariate_normal([2,3], [[1,0],[0,1]], 100)\n",
    "X1 = np.random.multivariate_normal([4,1], [[2,1],[1,2]], 100)\n",
    "X = np.concatenate([X0,X1])\n",
    "y = np.concatenate([np.zeros(100),np.ones(100)])\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[-1:8:.01, -3:8:.01]\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = model.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "contour = plt.contourf(xx, yy, probs, 25, alpha = 0.6,\n",
    "                      vmin=0, vmax=1)\n",
    "                      \n",
    "ax_c = plt.colorbar(contour)\n",
    "ax_c.set_label(\"$P(y = 1)$\")\n",
    "ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "plt.scatter(X0[:,0],X0[:,1])\n",
    "plt.scatter(X1[:,0],X1[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use the sigmoid function?\n",
    "\n",
    "It is very natural to ask why we use the sigmoid function instead of some other \"S\"-shaped curve.  There are a few reasons:\n",
    "\n",
    "*  There is a very nice theory of \"generlized linear models\".  In brief:\n",
    "    * We assume that the distribution $P(y | \\vec{x})$ has a particular form which fits into an \"exponential family\" of distributions (in our case the Bernoulli distributions).\n",
    "    * We specify a \"link function\" $g$ with $g(\\mathbb{E}(y | \\vec{x})) = \\vec{x} \\cdot \\vec{\\beta}$ (i.e. the link of the expected value is a linear function of the predictors).\n",
    "    * In this setup we have a \"canonical link function\" (a bit technical to define).  For the Bernoulli family of distributions, $g(p) = \\log(\\frac{p}{1-p})$ is the canonical link function, and $g^{-1} = \\sigma$.\n",
    "        * The canonical link makes the calculus come out really nicely.  We saw in math hour that the gradient and hessian for logistic regression are extremely simple:  this generalizes to all generalized linear models.\n",
    "* As we have already noted, the parameters are easily interpretable.\n",
    "\n",
    "You *could* use other link functions.  A popular choice is the inverse of the cdf of the normal distribution $\\Phi^{-1}$ which gives us \"probit regression\": we assume $\\mathbb{E}(y | \\vec{x}) = \\Phi(\\vec{x} \\cdot \\vec{\\beta}$).\n",
    "* This can be interpretted as assuming that our data generating process is that $y = 1$ when $\\vec{x} \\cdot \\vec{\\beta} + \\epsilon > 0$ and $\\epsilon \\sim \\mathcal{N}(0,1)$.\n",
    "* $\\Phi$ and $\\sigma$ are good approximations of each other, so the results are often quite similar.\n",
    "* Probit regression *is* preferable sometimes, such as when we want to avoid an assumption of [independence of irrelevant alternatives](https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives).\n",
    "* The interpretation of the model parameters is harder.\n",
    "* The calculus of fitting the model is a lot harder (no nice closed form formulas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Assumptions\n",
    "\n",
    "While we were explaining the concept of logistic regression, we did not mention any of the assumptions of the algorithm. Let's talk about that here before we move on to real data.\n",
    "\n",
    "- The log odds depend linearly on the regressors.\n",
    "- The regressors are measured without error.\n",
    "- The observations are independent.\n",
    "- The data are not perfectly linearly seperable (i.e. there cannot be some vector $\\vec{v}$ with $\\vec{x} \\cdot \\vec{v} < 0$ for one class and $>0$ for the other class).\n",
    "    - If they are, we will have $\\vert \\beta \\vert \\to \\infty$ as we train the model.  The resulting $p_\\beta$ will approach the indicator function of the set $\\{\\vec{x}: \\vec{x} \\cdot \\vec{v} > 0\\}$ as we train the model.  This isn't *horrible*, but is probably better to use something like a support vector machine in this case.\n",
    "- We do not have multicollinearity of the regressors.\n",
    "    - Just as in linear regression, multicolinearity will not impact our ability to accurately predict probabilities.  It only prevents us from understanding feature importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Heavily modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
