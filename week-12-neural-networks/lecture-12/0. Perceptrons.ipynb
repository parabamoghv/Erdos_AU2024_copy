{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc599610",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "Perceptrons are the fundamental building block of neural networks. They were introduced by Rosenblatt in 1960, <a href=\"https://ieeexplore.ieee.org/abstract/document/4066017\">https://ieeexplore.ieee.org/abstract/document/4066017</a>.\n",
    "\n",
    "The material in this notebook is adapted from the text <a href=\"https://link.springer.com/content/pdf/10.1007/978-3-319-94463-0.pdf\">Neural Networks and Deep Learning</a> by Charu C. Aggarawal.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss the foundation of the neural network, the perceptron,\n",
    "- Demonstrate the fundamental limitation of the perceptron and\n",
    "- Show how to build a perceptron in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668a328",
   "metadata": {},
   "source": [
    "## What is an artificial neural network?\n",
    "\n",
    "Neural networks are a technique that that loosely tries to mimic the network of neurons that make up brains. The idea being that we are trying to create learning algorithms that copy in some very loose sense how humans learn.\n",
    "\n",
    "The building blocks of the vast complex network of the brain is a single neuron, while the building blocks of an artificial neural network is a <i>perceptron</i>.\n",
    "\n",
    "We start at the beginning with this simple building block and then expand into slightly more complex neural networks in the next notebook.\n",
    "\n",
    "## The perceptron\n",
    "\n",
    "We will learn in the setting of a classification problem, but be aware that neural nets can be applied in a variety of settings.\n",
    "\n",
    "Suppose we have $n$ observations of $m$ features stored in $m$ $n$ by $1$ column vectors, $X_1, X_2, \\dots, X_m$, and let $X = \\left(X_1 | X_2 | \\dots |X_m\\right)$, and we want to predict some target $y$. <i>Note: we can include a column of $1$s, but we will leave it out for this formulation. When this is done we say that the perceptron has a bias term.</i>\n",
    "\n",
    "Let $\\sigma$ be some nonlinear function from $\\mathbb{R} \\rightarrow \\mathbb{R}$. For classification we take $\\sigma = \\text{sgn}$, the sign function, i.e. $\\sigma(x) = 1$ if $x>0$ and $\\sigma(x) = -1$ if $x<0$. In the language of neural networks we call $\\sigma$ an activation function.\n",
    "\n",
    "Perceptrons make an estimation of $y$, called $\\hat{y}$, like so\n",
    "$$\n",
    "\\hat{y} = \\sigma(w_1 X_1 + w_2  X_2 + \\dots + w_m X_m) = \\sigma(Xw),\n",
    "$$\n",
    "where in a potential abuse of notation I take $\\sigma(Xw)$ to mean $\\sigma$ applied to each of the $n$ entries of $Xw$, and $w=\\left(w_1, w_2, \\dots, w_m\\right)^T$.\n",
    "\n",
    "If I let $x=\\left(x_1,x_2,\\dots,x_m\\right)$ denote a single observation then I can picture this like so.\n",
    "\n",
    "<img src = \"perceptron.png\" width=\"50%\"></img>\n",
    "\n",
    "This sort of diagram is known as the <i>architecture</i> of the neural network. <i>A perceptron with a bias term would have a \"bias\" node added whose weight is denoted with a $b$.</i>\n",
    "\n",
    "The column of nodes are referred to as the input layer because these are the inputs into the perceptron, the output node has both $\\Sigma$ and $\\sigma$ because this is where our weighted sum ($\\Sigma$) and nonlinear transformation ($\\sigma$) occurs.  \n",
    "\n",
    "Note the while we did not include a bias term (adding a constant to $Xw$) this can easily be done, we just left it out for simplicity.\n",
    "\n",
    "### Finding the $w$\n",
    "\n",
    "So how do we find the weights?\n",
    "\n",
    "First the weights are randomly selected. Then you use a single data point from the training set, $X^{(i)}$, and you calculate the error, $y^{(i)}-\\hat{y}^{(i)}$.\n",
    "\n",
    "You then update $w$ like so:\n",
    "$$\n",
    "w_{\\text{update}} = w_{\\text{current}} + \\alpha (y^{(i)}-\\hat{y}^{(i)}) X^{(i)} \n",
    "$$\n",
    "\n",
    "$\\alpha$ is the learning rate of the network.\n",
    "\n",
    "The perceptron will cycle through all of the training points and continue to adjust the weights until it converges to a weight vector $w$. Each cycle through the training set is called an <i>epoch</i>. \n",
    "\n",
    "Typically the training points are chosen at random without replacement.\n",
    "\n",
    "Note, that this can be performed small batches of training points, at which point the batches are chosen randomly without replacement.\n",
    "\n",
    "Also note that the algorithm can be rewritten to work in parallel, <a href=\"http://www.cs.cmu.edu/~wcohen/10-605/parallel-perceptrons.pdf\">http://www.cs.cmu.edu/~wcohen/10-605/parallel-perceptrons.pdf</a> and <a href=\"https://www.sciencedirect.com/science/article/pii/S0167819106800177\">https://www.sciencedirect.com/science/article/pii/S0167819106800177</a>.\n",
    "\n",
    "\n",
    "Also for a very simple example of training the perceptron check out this reference starting on page 10, <a href=\"http://hagan.okstate.edu/4_Perceptron.pdf\">http://hagan.okstate.edu/4_Perceptron.pdf</a>.\n",
    "\n",
    "## Perceptrons in `sklearn`\n",
    "\n",
    "Let's now get some brief practice implementing a perceptron in `sklearn`. We do not spend much time here for reasons that will become clear.\n",
    "\n",
    "Here are the docs on `sklearn`'s implementation of perceptrons, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perceptron is stored in sklearn.linear_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d4590",
   "metadata": {},
   "source": [
    "We will use the perceptron to build a classifier on this phony data set. We will just use the defaults from the `sklearn` Perceptron object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f2b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate some random input data\n",
    "np.random.seed(324)\n",
    "\n",
    "X = np.random.random((200,2))\n",
    "\n",
    "## When x_1 - x_2 < -.05 classify as -1\n",
    "## when x_1 - x_2 > -.05 classify as 1\n",
    "y = np.zeros(200)\n",
    "y[X[:,0] - X[:,1] > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "\n",
    "plt.scatter(X[y==1, 0],\n",
    "            X[y==1, 1],\n",
    "            c='orange',\n",
    "            marker='v',\n",
    "            label = \"Class 1\")\n",
    "plt.scatter(X[y==0, 0],\n",
    "            X[y==0, 1],\n",
    "            c='blue',\n",
    "            label = \"Class 0\")\n",
    "\n",
    "plt.plot(np.linspace(0,1,100), \n",
    "         np.linspace(0,1,100),\n",
    "         'k--',\n",
    "         label = \"Decision Boundary\")\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac167c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We make a perceptron object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d081a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We fit the object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1, xx2 = np.meshgrid(np.arange(-.01, 1.01, .01),\n",
    "                          np.arange(-.01, 1.01, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "preds = perc.predict(X_pred)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.1,\n",
    "            c='lightblue',\n",
    "            s=50)\n",
    "plt.scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.1,\n",
    "            c='orange',\n",
    "            s=50)\n",
    "\n",
    "\n",
    "plt.scatter(X[y==0,0],\n",
    "            X[y==0,1],\n",
    "            s=50,\n",
    "            c='b',\n",
    "            label = \"Class 0\",\n",
    "            edgecolor='k')\n",
    "plt.scatter(X[y==1,0],\n",
    "            X[y==1,1],\n",
    "            s=50,\n",
    "            c='darkorange',\n",
    "            marker = 'v',\n",
    "            label = \"Class 1\",\n",
    "           edgecolor='k')\n",
    "\n",
    "plt.plot(np.linspace(0,1,100), np.linspace(0,1,100),'k--',label = \"Decision Boundary\")\n",
    "\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94a7fe",
   "metadata": {},
   "source": [
    "Let's look at another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "y = np.array([1,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(X[y == 1,0],\n",
    "            X[y == 1,1], \n",
    "            c = 'orange', \n",
    "            marker='v', \n",
    "            label = \"1\", \n",
    "            s=100)\n",
    "plt.scatter(X[y == 0,0],\n",
    "            X[y == 0,1], \n",
    "            c = 'blue', \n",
    "            label = \"0\", \n",
    "            s=100)\n",
    "\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67ee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a model object\n",
    "# call the variable p\n",
    "\n",
    "\n",
    "# Fit the model here\n",
    "\n",
    "\n",
    "# Make a prediction\n",
    "print(\"Actual data\", y)\n",
    "print(\"Predicted Data\", p.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1, xx2 = np.meshgrid(np.arange(-.01, 1.01, .01),\n",
    "                          np.arange(-.01, 1.01, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "preds = p.predict(X_pred)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.1,\n",
    "            c='lightblue',\n",
    "            s=50)\n",
    "plt.scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.1,\n",
    "            c='orange',\n",
    "            s=50)\n",
    "\n",
    "\n",
    "plt.scatter(X[y==0,0],\n",
    "            X[y==0,1],\n",
    "            s=50,\n",
    "            c='b',\n",
    "            label = \"Class 0\",\n",
    "            edgecolor='k')\n",
    "plt.scatter(X[y==1,0],\n",
    "            X[y==1,1],\n",
    "            s=50,\n",
    "            c='darkorange',\n",
    "            marker = 'v',\n",
    "            label = \"Class 1\",\n",
    "           edgecolor='k')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33752b97",
   "metadata": {},
   "source": [
    "##### How did it do?\n",
    "\n",
    "<i>Not good</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730ae08",
   "metadata": {},
   "source": [
    "A single perceptron is not capable of separating data sets that are not linearly separable. This severely hurt interest in the method back in the 1950s and 60s, <a href=\"https://en.wikipedia.org/wiki/Perceptron\">https://en.wikipedia.org/wiki/Perceptron</a>. \n",
    "\n",
    "However, if your data is linearly separable there is a proof that guarantees the perceptron will converge as well as an upper bound on the number of epochs it must endure to get there, see the previous wikipedia link.\n",
    "\n",
    "The linear limitation is precisely why we now end our time with the perceptron and move on to more complicated neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea756e8",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9cb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
