{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Networks\n",
    "\n",
    "The limitation on perceptrons to linear decision boundaries stymied neural network development. However, eventually (the 1980s I believe) there was a break through that found an architecture that does allow for nonlinear decision boundaries, multilayer networks.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce the multilayer network architecture,\n",
    "- Demystify <i>backpropagation</i> and\n",
    "- Demonstrate how to implement a multilayer network in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward network architecture\n",
    "\n",
    "The class of multilayer networks we examine in this notebook are known as <i>feed forward networks</i>. Let's first show an image and then use that image for our explanation.\n",
    "\n",
    "<img src=\"multilayer.png\" width=\"60%\"></img>\n",
    "\n",
    "As you might be able to see they are called feed forward networks because each layer feeds directly into the next one.\n",
    "\n",
    "In particular we have drawn a feed forward network with $2$ <i>hidden layers</i> each with dimension $3$, we call these hidden layers because we only see what is put into the input layer and what comes out of the output layer. So in some sense what goes on in the middle layers is \"hidden\" to us.\n",
    "\n",
    "Note that neural networks can have more complex architectures. We touch on these in later notebooks.\n",
    "\n",
    "## Mathematical setup\n",
    "\n",
    "Suppose that we have $n$ observations of $m$ features, let $x$ represent a single observation as an $m$ by $1$ vector. Further, suppose we have $k$ hidden layers and that layer $l$ has $p_l$ nodes within it, and take $h_l$ to denote the vector corresponding to hidden layer $l$. Also suppose that the output layer has $o$ nodes. Let $W_1$ be a $p_1$ by $m$ weight matrix, for $l = 2,\\dots, k$ let $W_l$ be a $p_l$ by $p_{l-1}$ weight matrix, and let $W_{k+1}$ be an $o$ by $p_k$ weight matrix. Finally take $\\Phi$ to be some activation function.\n",
    "\n",
    "Then we can set up how the output of the network is calculated with these recursively defined equations:\n",
    "$$\n",
    "\\begin{array}{l l r}\n",
    "h_1 =  & \\Phi (W_1 x) & \\text{Input to Hidden Layer }1 \\\\\n",
    "h_{l+1} = & \\Phi (W_{l+1} h_l) \\ \\forall l = 1, 2, \\dots, k-1 & \\text{Hidden Layer } l \\text{ to Hidden Layer } l+1 \\\\\n",
    "\\hat{y} = & \\Phi(W_{k+1} h_k) & \\text{Hidden Layer } k \\text{ to Output Layer}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For now we stick with an abstract activation function, but we will mention a number of options before ending the notebook.\n",
    "\n",
    "Sometimes you may see architecture diagrams that represent the nodes as giant rectangles meant to represent the vectors in the recursive equations.\n",
    "\n",
    "<img src=\"multilayer2.png\" width=\"60%\"></img>\n",
    "\n",
    "### Fitting the multilayer network, <i>backpropagation</i>\n",
    "\n",
    "<i>Backpropagation</i> is how we find the optimal weight vector, $w$, it is really just a fancy name for the chain rule mixed with gradient descent. Let's demonstrate with a simple architecture.\n",
    "\n",
    "<img src=\"simple.png\" width=\"60%\">\n",
    "\n",
    "In this architecture we have: \n",
    "\n",
    "$$\n",
    "h_1 = \\Phi (w_1 x_1),\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\Phi (w_2 h_1),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\Phi(w_3 h_2).\n",
    "$$\n",
    "\n",
    "Backpropagation consists of a <i>forwards step</i> and a <i>backwards step</i>.\n",
    "\n",
    "\n",
    "#### The forwards step\n",
    "\n",
    "Let $w = \\left(w_1, w_2, w_3\\right)^T$. As with the perceptron we initialize $w$ with random weights. Then run a randomly selected training point, $x^{(i)}$ through the network getting the values for each layer of the network along the way.\n",
    "\n",
    "So when the forward step is completed you have a $\\hat{y}$ and $\\hat{h}$s for each layer of the network.\n",
    "\n",
    "#### The backwards step\n",
    "\n",
    "The backwards step is how we update our weights, $w$. Let our cost function be $C = (\\hat{y} - y)^2$.\n",
    "\n",
    "In order to update $w$ we use gradient descent, so $w_{\\text{new}} = w_\\text{old} - \\eta \\nabla C(w_\\text{old})$, where the gradient is taken with respect to $w$, $\\eta$ is a hyperparameter called the learning rate, and for the purposes of our derivation we assume that $C$ is differentiable with respect to all of the weights (there are work arounds for activation functions that aren't differentiable everywhere).\n",
    "\n",
    "The backwards step is where backpropagation gets its name, as we'll see now.\n",
    "\n",
    "Using the chain rule we can find $\\frac{\\partial C}{\\partial w_1}, \\frac{\\partial C}{\\partial w_2}, $ and $\\frac{\\partial C}{\\partial w_3}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_3} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_3} = 2\\left( \\hat{y} - y \\right) \\Phi'(w_3 h_2) h_2\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_2} \\frac{\\partial h_2}{\\partial w_2} = 2(\\hat{y} - y) \\Phi'(w_3 h_2) w_3 \\Phi'(w_2 h_1) h_1\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w_1} = \\frac{\\partial C}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial h_2} \\frac{\\partial h_2}{\\partial h_1} \\frac{\\partial h_1}{\\partial w_1} = 2(\\hat{y} - y) \\Phi'(w_3 h_2) w_3 \\Phi'(w_2 h_1) w_2 \\Phi'(w_1 x_1^{(i)}) x_1^{(i)}\n",
    "$$\n",
    "\n",
    "In all of the above expressions we take each of the values to be the one we found in forwards step.\n",
    "\n",
    "##### The gradient adjustment\n",
    "\n",
    "Then before we randomly choose another training instance we update the weights by performing $w_{\\text{new}} = w_\\text{old} - \\eta \\nabla C(w_\\text{old})$\n",
    "\n",
    "##### Epochs\n",
    "\n",
    "This process is completed by cycling through all of the training points in some random order. Each cycle through the training set is called an <i>epoch</i>.\n",
    "\n",
    "#### That's it!\n",
    "\n",
    "While this was a simple example that's really all backpropagation is. The only thing that gets more complicated with more complex feed forward network architectures is the indexing, which can quickly become a massive headache.\n",
    "\n",
    "#### Common adjustments to the gradient descent\n",
    "\n",
    "Two common adjustments come to the gradient descent steps:\n",
    "\n",
    "1. Sometimes in order to speed up calculations on all of the training points you perform batch gradient descent in which small batches of points are run through the forwards step with the same $w$ and then for the update you use the average of the batch's backwards step.\n",
    "\n",
    "2. Instead of selecting $\\eta$ by hand you can let it be a random value for each step. The idea being it can help you get out of local minima of the cost function. This is known as <i>stochastic gradient descent</i>.\n",
    "\n",
    "We have a specific gradient descent notebook in the `Supervised Learning` lecture folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing in `sklearn`\n",
    "\n",
    "Let's now see how to implement this model in `sklearn` using the MNIST data set as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X,y = load_digits(return_X_y=True)\n",
    "X = X/255\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
    "                                                       test_size=.2,\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=124,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before learning how to make the model in `sklearn` we will make a validation split. This is a common step in neural network modeling because cross-validation may take too long to be practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_train, X_val, y_train_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                                 test_size=.2,\n",
    "                                                                   shuffle=True,\n",
    "                                                                   random_state=41,\n",
    "                                                                   stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn` you implement multilayer network classification with `MLPClassifier` and multilayer network regression with `MLPRegressor`.\n",
    "\n",
    "Here are the documentation pages for both:\n",
    "\n",
    "- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</a>\n",
    "\n",
    "- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\">https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this makes an mlp classifier with 1 hidden layer\n",
    "## this layer has 500 nodes, controlled by hidden_layer_sizes\n",
    "## we increase the maximum iterations for the gradient descent\n",
    "mlp1 = \n",
    "\n",
    "## Here's a second classifier with 2 hidden layers of 200 nodes each,\n",
    "## controlled by hidden_layer_sizes\n",
    "## we increase the maximum iterations for the gradient descent\n",
    "mlp2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the two classifiers\n",
    "mlp1.fit(X_train_train, y_train_train)\n",
    "mlp2.fit(X_train_train, y_train_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the accuracies for these two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The single hidden layer with 500 nodes has a training accuracy of\",\n",
    "         np.round(100*accuracy_score(y_train_train, mlp1.predict(X_train_train)),2))\n",
    "\n",
    "print(\"The single hidden layer with 500 nodes has a validation accuracy of\",\n",
    "         np.round(100*accuracy_score(y_val, mlp1.predict(X_val)),2))\n",
    "\n",
    "print(\"The two hidden layers with 200 nodes each has a training accuracy of\",\n",
    "         np.round(100*accuracy_score(y_train_train, mlp2.predict(X_train_train)),2))\n",
    "\n",
    "print(\"The two hidden layers with 200 nodes each has a validation accuracy of\",\n",
    "         np.round(100*accuracy_score(y_val, mlp2.predict(X_val)),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What architecture works best for you depends upon the problem you are dealing with. Typically you will have to do some sort of tuning process to find the <i>optimal</i> architecture.\n",
    "\n",
    "Recall for multiclass problems that we can also look at the confusion matrix to see if the mistakes are being made randomly or with some sort of pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Single hidden layer with 500 nodes\\n\",\n",
    "     \"+++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "pd.DataFrame(confusion_matrix(y_val, mlp1.predict(X_val)), \n",
    "                columns=[\"predicted \"+str(i) for i in range(10)],\n",
    "                index=[\"actual \"+str(i) for i in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Two hidden layers with 200 nodes each\\n\",\n",
    "     \"+++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "pd.DataFrame(confusion_matrix(y_val, mlp2.predict(X_val)), \n",
    "                columns=[\"predicted \"+str(i) for i in range(10)],\n",
    "                index=[\"actual \"+str(i) for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different activation functions\n",
    "\n",
    "Now let's briefly show the four activation functions, $\\Phi$ that are used by `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First the identity\n",
    "## which is technically not nonlinear but whatever\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.plot(np.linspace(-2,2,100),np.linspace(-2,2,100), linewidth = 3)\n",
    "\n",
    "plt.xlim(-2,2)\n",
    "\n",
    "plt.title(\"The Identity Activation\",fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the logistic function\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "plt.plot(x,y, linewidth = 3)\n",
    "\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.text(-9,.8,\"$y = 1/(1+e^{-x})$\", fontsize=12)\n",
    "\n",
    "plt.title(\"The Logistic Activation\",fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the hyperbolic tan function\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.tanh(x)\n",
    "\n",
    "plt.plot(x,y, linewidth = 3)\n",
    "\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.text(-9,.8,\"$y = tanh(x)$\", fontsize=12)\n",
    "\n",
    "plt.title(\"The Hyperbolic Tangent Activation\",fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the Rectified Linear Unit [ReLU] function\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = np.linspace(-10,10,100)\n",
    "y[x<0] = 0\n",
    "\n",
    "plt.plot(x,y, linewidth = 3)\n",
    "\n",
    "plt.xlim(-10,10)\n",
    "\n",
    "plt.text(-9,8,\"$y = \\max\\{x,0\\}$\", fontsize=12)\n",
    "\n",
    "plt.title(\"The Rectified Linear Unit (ReLU) Activation\",fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default in both `sklearn`'s `MLPRegressor` and `MLPClassifier` is ReLU.\n",
    "\n",
    "## Universal approximator\n",
    "\n",
    "It has been proven mathematically that \"a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\\mathbb{R}^n$, under mild assumptions on the activation function. The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters\", <a href=\"https://en.wikipedia.org/wiki/Universal_approximation_theorem\">https://en.wikipedia.org/wiki/Universal_approximation_theorem</a>.\n",
    "\n",
    "So we have done it! We have found the best machine learning algorithm and we should end here.\n",
    "\n",
    "Continuing on from wikipedia: \"however, it does not touch upon the algorithmic learnability of those parameters.\"\n",
    "\n",
    "Meaning that while yes we can theoretically approximate any reasonable function with a high enough dimensional single hidden layer feed forward network, this is not always practically possible.\n",
    "\n",
    "### Deep learning\n",
    "\n",
    "It has been found that you can trade in the height of a single hidden layer for increased depth and get similar results. Wanting to understand the possibilities and limitations of such architecture is where the field of deep learning comes from.\n",
    "\n",
    "Which leads us to the deficiencies of feed forward networks in practice.\n",
    "\n",
    "## Deficiencies\n",
    "\n",
    "1. Feed Forward Neural Nets can very easily overfit the training data. This can be controlled for with a variety of techniques.\n",
    "\n",
    "2. Gradients can vanish or explode when your networks get too deep because of the chain rule.\n",
    "\n",
    "3. Convergence can be slow and difficult.\n",
    "\n",
    "4. Cost functions often have many local minima that you can get stuck in when using normal gradient descent with a fixed learning rate.\n",
    "\n",
    "5. For complicated networks a normal laptop may not suffice and you'll need more powerful hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The following were extremely helpful in understanding multilayer neural nets.\n",
    "\n",
    "This entire 4 video youtube series, <a href=\"https://www.youtube.com/watch?v=aircAruvnKk\">https://www.youtube.com/watch?v=aircAruvnKk</a>.\n",
    "\n",
    "This blog post, <a href=\"https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a>.\n",
    "\n",
    "This online book <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\">http://neuralnetworksanddeeplearning.com/chap2.html</a>.\n",
    "\n",
    "And the book mentioned in the perceptron notebook <a href=\"https://link.springer.com/content/pdf/10.1007/978-3-319-94463-0.pdf\">https://link.springer.com/content/pdf/10.1007/978-3-319-94463-0.pdf</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
