{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recurrent Neural Networks\n",
    "\n",
    "The final neural network architecture we will cover is the recurrent neural network.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Discuss the kinds of problems recurrent nets are designed for,\n",
    "- Give an overview of basic RNN architectures,\n",
    "- Demonstrate the weighted sum setup for such architectures and\n",
    "- Build a RNN to predict IMDB review sentiment.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential data\n",
    "\n",
    "Similar to how convolutional neural networks were designed to deal with grid-based data, recurrent neural networks (RNN) were built to deal with sequential data. Some examples of sequential data include:\n",
    "- Time series,\n",
    "- Natural language,\n",
    "- Music\n",
    "- and More.\n",
    "\n",
    "In sequential data it is often the case that the past impacts the future. For example, in text data preceding words may have an impact on our understanding of the current word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN architecture\n",
    "\n",
    "RNNs actually have a wide array of architectures depending upon the specific problem you are trying to solve. To demonstrate the general idea we will consider this simple architecture:\n",
    "\n",
    "<img src=\"rnn1.png\" width=\"40%\"></img>\n",
    "\n",
    "Here we have a RNN built to take in sequential data $(X^{(i)}, y^{(i)})$ four steps at a time, with four corresponding hidden layers. A use case of such an architecture would be an autofill predictor that takes in four words and then predicts what the next word will be. In this example, the $X^{(i)}$ and $y^{(i)}$ would be one-hot encoded vectors of length $d$, where $d$ is the size of the <i>lexicon</i>.\n",
    "\n",
    "Each \"step\" of the architecture's sequence actually represents a unique feed forward network like so:\n",
    "\n",
    "<img src=\"rnn2.png\" width=\"50%\"></img>\n",
    "\n",
    "Where hidden layer $t+1$ also takes inputs from hidden layer $t$. In this way the final hidden layer takes input from all the previous hidden layers in a manner similar to a moving average model from our `Time Series` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted sum set up\n",
    "\n",
    "The values for each hidden layer's nodes in this network are given by:\n",
    "\n",
    "$$\n",
    "h_1 = \\Phi \\left(W_{xh} X^{(1)}\\right) \\ \\text{ and } \\ h_t = \\Phi \\left( W_{xh}X^{(t)} + W_{hh} h_{t-1} \\right),\n",
    "$$\n",
    "\n",
    "where $W_{xh}$ and $W_{hh}$ have the same entries regardless of the hidden layer considered, and the predicted value for each node of the output layers is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y^{(t)}} = \\sigma\\left( W_{hy} h_t \\right),\n",
    "$$\n",
    "\n",
    "where the entries of $W_{hy}$ are the same regardless of $t$ and the nonlinear activation functions, $\\Phi$ and $\\sigma$ could be different or the same.\n",
    "\n",
    "The dimensions of each $W_*$ weight matrix depend upon the dimensions of the $X^{(t)}$ and $y^{(t)}$ vectors and  the size of the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: IMDB sentiment analysis\n",
    "\n",
    "As an illustrative example we will use `keras` to build a sentiment classifier using IMDB movie reviews. Let's first load this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The data is stored in here\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will determine the number of vocab words in our\n",
    "## dictionary\n",
    "max_features = 10000\n",
    "\n",
    "## num_words tells keras to return the reviews so they contain only\n",
    "## the num_words most used words across all the reviews\n",
    "(X_train, y_train), (X_test,y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "## Note you may receive a warning, this is not your fault, and is due to how\n",
    "## keras is loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at the first training observation\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as a list of indices, each of which is representative of a word. Let's see what this particular review looks like, once we have translated it from indices to words. Do not focus on the following code for now, as it is not important for building the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key,value) in word_index.items()])\n",
    "\n",
    "## The first training review, where words outside the top 1000 are replaced with\n",
    "## ? marks\n",
    "print(\" \".join([reverse_word_index.get(i-3, '?') for i in X_train[0]]))\n",
    "print()\n",
    "print(\"sentiment value =\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The review above had a $y$ value of $1$, meaning that it has positive sentiment. A value of $0$ indicates a negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our network architecture\n",
    "\n",
    "The RNN architecture we will use for this problem is pictured below:\n",
    "\n",
    "<img src=\"rnn3.png\" width=\"60%\"></img>\n",
    "\n",
    "where $s$ denotes the length of the sequence we will consider.\n",
    "\n",
    "In this problem we have a single output at the end of the sequence because we are only predicting whether the entire sequence represents a positive or negative review. This example demonstrates how different sequential prediction tasks can correspond to slightly different network architectures.\n",
    "\n",
    "Let's now turn our data into sequences and then build our RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import pad_sequences\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of our Xs prior to becoming sequences\")\n",
    "print(\"X_train:\", np.shape(X_train))\n",
    "print(\"X_test:\", np.shape(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In order to fit a neural net to these observations\n",
    "## we need all of our reviews to be sequences of the same length\n",
    "## We'll set a fixed length of 100 words. For those shorter than 100\n",
    "## words we'll add padding. For those longer than 100 words, we'll cut off the\n",
    "## excess\n",
    "## keras does this for us :)\n",
    "max_length = 100\n",
    "\n",
    "## pad_sequences(data, maxlen)\n",
    "X_train_seq = \n",
    "X_test_seq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape of our Xs after becoming sequences\")\n",
    "print(\"X_train:\", np.shape(X_train_seq))\n",
    "print(\"X_test:\", np.shape(X_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note the padding at the beginning \n",
    "X_train_seq[100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making our validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_train,X_val,y_train_train,y_val = train_test_split(X_train_seq, y_train,\n",
    "                                                           test_size=.2,\n",
    "                                                           shuffle=True,\n",
    "                                                           stratify = y_train,\n",
    "                                                           random_state=440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Network\n",
    "\n",
    "This network will introduce two new layer types `Embedding` and `SimpleRNN`. \n",
    "\n",
    "For now all you will need to know about the `Embedding` layer is that it takes in the sequences we just generated and turns them into a vector representation of the sequence that is more useful to the network. One way to think of this is the `Embedding` layer is similar to running the data through a PCA type step prior to fitting the network. The exact details are slightly beyond the scope of our boot camp and better suited for a natural language processing course, here are the docs on the `Embedding` layer, <a href=\"https://keras.io/api/layers/core_layers/embedding/\">https://keras.io/api/layers/core_layers/embedding/</a>. <i>Note: the `Embedding` layer is specific to NLP tasks, and not used if you are building a RNN on time series data. This is a preprocessing step.</i>\n",
    "\n",
    "The `SimpleRNN` layer is the akin to the RNN architecture we described above. Here are the docs <a href=\"https://keras.io/api/layers/recurrent_layers/simple_rnn/\">https://keras.io/api/layers/recurrent_layers/simple_rnn/</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the keras stuff we'll need\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Make the model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Add the layers we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding is added first\n",
    "## the size of our dictionary is the first input, max_features\n",
    "## followed by the desired embedding size, I chose 32, again for no particular reason\n",
    "model.add()\n",
    "\n",
    "## Then the SimpleRNN layer\n",
    "## the first input should be the same size as the input layer, i.e. 32\n",
    "## return_sequences determines if the network should return the hidden state value for \n",
    "## each hidden layer h1, h2, ..., hT\n",
    "## Since we only want the last one, hT, we set it equal to False\n",
    "## If we were building a network with multiple SimpleRNN layers,\n",
    "## you'd set this to True\n",
    "model.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally we'll add a Dense Layer for classification\n",
    "## purposes\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Compile the model\n",
    "\n",
    "Again we need to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## notice we use binary_crossentropy here\n",
    "## this is because our sentiment problem is a binary\n",
    "## classification problem\n",
    "model.compile(optimizer='rmsprop',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(X_train_train, y_train_train,\n",
    "                    epochs = epochs,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val,y_val))\n",
    "\n",
    "## Note training this model can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the training and validation accuracy\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,epochs+1), history_dict['accuracy'], label = \"Training Accuracy\")\n",
    "plt.scatter(range(1,epochs+1), history_dict['val_accuracy'], label = \"Validation Set Accuracy\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Accuracy\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the training and validation loss\n",
    "plt.figure(figsize = (8,6))\n",
    "\n",
    "plt.scatter(range(1,epochs+1), history_dict['loss'], label = \"Training Loss\")\n",
    "plt.scatter(range(1,epochs+1), history_dict['val_loss'], label = \"Validation Set Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss Function Value\", fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "RNNs can be used for a variety of sequential data problems that we do not have time to cover in depth.\n",
    "\n",
    "Moreover, `SimpleRNN` is actually a seldom used RNN layer type because there are more complicated `keras` layers that tend to outperform `SimpleRNN`. \n",
    "\n",
    "For those interested in learning more about RNN here are two references I leaned on heavily in making this notebook:\n",
    "- <a href=\"http://faculty.neu.edu.cn/yury/AAI/Textbook/Deep%20Learning%20with%20Python.pdf\">Deep Learning with Python</a>, which is a practical guide on how to implement neural networks with `keras`. RNNs start in chapter 6.\n",
    "- <a href=\"https://link.springer.com/book/10.1007/978-3-319-94463-0\">Neural Networks and Deep Learning</a>, which is a textbook that builds up the theory of neural networks. Recurrent neural networks start in chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
