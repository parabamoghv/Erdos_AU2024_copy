{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd17e886",
   "metadata": {},
   "source": [
    "# Bayes' Based Classifiers\n",
    "\n",
    "We will now introduce a series of classification algorithms based on Bayes' rule from probability theory.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Review Bayes' rule,\n",
    "- Introduce a classification model framework for the three models we consider,\n",
    "- Reintroduce the iris data set and\n",
    "- Demonstrate:\n",
    "    - Linear discriminant analysis,\n",
    "    - Quadratic discriminant analysis and\n",
    "    - Na&#xEF;ve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfa46c",
   "metadata": {},
   "source": [
    "## Review of Bayes' rule\n",
    "\n",
    "Assume that we have some probability space, $\\Omega$.\n",
    "\n",
    "### Conditional probability\n",
    "\n",
    "Remember that for events $A$ and $B$ with $P(B)\\neq0$ we define the probability of $A$ conditional on $B$ as:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A\\cap B)}{P(B)}.\n",
    "$$\n",
    "\n",
    "This definition can be visualized with this graphic:\n",
    "\n",
    "<img src=\"lecture_9_assets/cond_prob.png\" width=\"40%\"></img>\n",
    "\n",
    "### Law of total probability\n",
    "\n",
    "If $B_1, \\ B_2, \\dots, B_n$ are disjoint events such that $\\cup_{i=1}^n B_i= \\Omega$, then it holds that:\n",
    "\n",
    "$$\n",
    "P(A) = \\sum_{i=1}^n P(A \\cap B_i),\n",
    "$$\n",
    "\n",
    "for any event $A$.\n",
    "\n",
    "The law of total probability can be visualized with this graphic:\n",
    "\n",
    "<img src=\"lecture_9_assets/tot_prob.png\" width=\"40%\"></img>\n",
    "\n",
    "### Bayes' rule\n",
    "\n",
    "For events $A$ and $B$ with $P(B)\\neq0$, then Bayes' rule (or the Bayes–Price theorem) is\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B)}.\n",
    "$$\n",
    "\n",
    "This is sometimes taken a step further using the law of total probability for example:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B \\cap A) + P(B \\cap A^c)} = \\frac{P(B|A) P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)},\n",
    "$$\n",
    "\n",
    "where $A^c = \\Omega - A$.\n",
    "\n",
    "For a nice visualization of conditional probability and Baye's rule check out this blog post, <a href=\"https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/\">https://oscarbonilla.com/2009/05/visualizing-bayes-theorem/</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20096c4f",
   "metadata": {},
   "source": [
    "## Using Bayes' rule for classification\n",
    "\n",
    "Suppose we have a set of $p$ features collected in a random vector $X$ and a categorical output variable $y$ that can take on any of $\\mathcal{C}$ possible categories.\n",
    "\n",
    "We need a version of Bayes' rule for probability distribution functions of continuous parameters, which is more technical to prove, so we just give the statement here.  It follows the same general schema.\n",
    "\n",
    "Let $f_k(X)$ be the pdf of the features conditioned on $y$ being class $k$ and let $\\pi_k = P(y= k)$.\n",
    "\n",
    "Then we have\n",
    "\n",
    "$$\n",
    "P(y=j|X=X^*) = \\frac{\\pi_j f_j(X^*)}{\\displaystyle \\sum\\limits_{k=1}^\\mathcal{C} \\pi_k f_k(X^*)}\n",
    "$$\n",
    "\n",
    "We can estimate $\\pi_k$ as the fraction of observations which are of class $k$.  Different assumptions about the conditional distributions $f_k(X)$ lead to different classification algorithms.\n",
    "\n",
    "Note that these algorithms will all model the full joint distribution of $X$ and $y$, which makes them \"generative\" algorithms.  We can contrast this with a \"discriminative\" algorithm like logistic regression which only models $P(y = k | X = X^*)$.  A generative algorithm allows us to generate new examples by sampling from the full joint distribution.\n",
    "\n",
    "### A return to the `iris` data set\n",
    "\n",
    "In this notebook we will illustrate the algorithms with our trusty `iris` data set, which provides four measurements (sepal length, sepal width, petal length and petal width) for $150$ irises of three distinct types ($50$ for each type).\n",
    "\n",
    "We load this data set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e065dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0579f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = iris['data']\n",
    "X = X.rename(columns={'sepal length (cm)':'sepal_length',\n",
    "                         'sepal width (cm)':'sepal_width',\n",
    "                         'petal length (cm)':'petal_length',\n",
    "                         'petal width (cm)':'petal_width'})\n",
    "y = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed567d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.copy(), y,\n",
    "                                                       shuffle=True,\n",
    "                                                       random_state=413,\n",
    "                                                       test_size=.2,\n",
    "                                                       stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                s=60,\n",
    "                label='$y=0$')\n",
    "plt.scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                s=60,\n",
    "                marker = 'v',\n",
    "                label='$y=1$')\n",
    "plt.scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                s=60,\n",
    "                marker = 'x',\n",
    "                label='$y=2$')\n",
    "\n",
    "plt.xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "plt.ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b7a1e",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis (LDA)\n",
    "\n",
    "The first model we discuss is know as <i>linear discriminant analysis</i> or LDA. Note that LDA is an ambiguous acronym in data science/machine learning because it can also stand for latent Dirichlet allocation, but in these notes we will use it to refer to linear discriminant analysis.\n",
    "\n",
    "### Model assumption\n",
    "\n",
    "In LDA we will assume that $(X|y=c)$ is Gaussian. What that means is dependent upon the number of features $p$.\n",
    "\n",
    "#### A single feature, $p=1$\n",
    "\n",
    "For a single feature we assume that \n",
    "\n",
    "$$\n",
    "f_c(X) = \\frac{1}{\\sqrt{2\\pi}\\sigma_c} \\exp \\left( -\\frac{1}{2\\sigma_c^2} (X - \\mu_c)^2 \\right),\n",
    "$$\n",
    "\n",
    "which is the probability density function of a normal random variable with mean $\\mu_c$ and standard deviation $\\sigma_c$. In linear discriminant analysis we assume that $\\sigma_1 = \\sigma_2 = \\dots = \\sigma_\\mathcal{C} = \\sigma$.\n",
    "\n",
    "This assumption leads to the following estimate of $P(y=c|X)$\n",
    "\n",
    "$$\n",
    "P(y=c|X) = \\frac{\\pi_c \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(X-\\mu_c)^2\\right)}{\\sum\\limits_{l=1}^\\mathcal{C} \\pi_l \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(X-\\mu_l)^2\\right)}.\n",
    "$$\n",
    "\n",
    "In this setting we would estimate $\\mu_c$ and $\\sigma$ with the following formulae:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}_c = \\frac{1}{n_c} \\sum_{i:y_i = c} X_i,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n-\\mathcal{C}} \\sum_{c=1}^\\mathcal{C} \\sum_{i:y_i = c}(X_i - \\hat{\\mu}_c)^2.\n",
    "$$\n",
    "\n",
    "#### Making classifications\n",
    "\n",
    "We typically make classifications in this setting by choosing the class $c$ for which $P(y=c|X)$ is largest. Through some algebra and $\\log$ manipulations you can show that this is equivalent to choosing the class, $c$, for which the <i>discriminant function</i> is largest where:\n",
    "\n",
    "$$\n",
    "\\delta_c = X \\frac{\\mu_c}{\\sigma^2} - \\frac{\\mu_c^2}{2\\sigma^2} + \\log\\left(\\pi_c\\right)\n",
    "$$\n",
    "\n",
    "is the discriminant function for class $c$. <i>We estimate this function with the $\\hat{\\mu}_c$ and $\\hat{\\sigma}$</i>\n",
    "\n",
    "Let's apply this to our iris data set. We will use petal length as our single feature for this LDA model.\n",
    "\n",
    "LDA is implemented in `sklearn` with `LinearDiscriminantAnalysis`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\">https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1588a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import linear discriminant analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1305ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model object\n",
    "\n",
    "## Fit the object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstrate predict_proba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca86559a",
   "metadata": {},
   "source": [
    "#### Demonstrating what LDA is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining the discriminant function\n",
    "def delta_c(x, mu_hat, sigma_hat_sq, pi_c):\n",
    "    return x*(mu_hat/sigma_hat_sq) - mu_hat**2/(2*sigma_hat_sq) + np.log(pi_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we estimate the means of the three normal distributions\n",
    "mu_0_hat = np.mean(X_train.loc[y_train==0].petal_length)\n",
    "mu_1_hat = np.mean(X_train.loc[y_train==1].petal_length)\n",
    "mu_2_hat = np.mean(X_train.loc[y_train==2].petal_length)\n",
    "\n",
    "## Then the common variance\n",
    "sigma_hat_sq = np.sum(np.power(X_train.loc[y_train==0].petal_length - mu_0_hat,2))\n",
    "sigma_hat_sq = sigma_hat_sq + np.sum(np.power(X_train.loc[y_train==1].petal_length - mu_1_hat,2))\n",
    "sigma_hat_sq = sigma_hat_sq + np.sum(np.power(X_train.loc[y_train==2].petal_length - mu_2_hat,2))\n",
    "sigma_hat_sq = sigma_hat_sq/(len(y_train)-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4bb8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot the sample distributions\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.hist(X_train.loc[y_train==0].petal_length,alpha=.6, \n",
    "         label=\"y=0\",\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(X_train.loc[y_train==1].petal_length,alpha=.6, \n",
    "         label=\"y=1\",\n",
    "         hatch = \"\\\\\",\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(X_train.loc[y_train==2].petal_length,alpha=.6, \n",
    "         label=\"y=2\",\n",
    "         hatch='//',\n",
    "         edgecolor=\"black\")\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.title(\"The Actual Sample Distributions\", fontsize=14)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Plot the fit normal distributions\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.hist(np.sqrt(sigma_hat_sq)*np.random.randn(1000)+mu_0_hat,alpha=.6, \n",
    "         label=\"y=0\",\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(np.sqrt(sigma_hat_sq)*np.random.randn(1000)+mu_1_hat,alpha=.6, \n",
    "         label=\"y=1\",\n",
    "         hatch='\\\\',\n",
    "         edgecolor=\"black\")\n",
    "plt.hist(np.sqrt(sigma_hat_sq)*np.random.randn(1000)+mu_2_hat,alpha=.6, \n",
    "         label=\"y=2\",\n",
    "         hatch='//',\n",
    "         edgecolor=\"black\")\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.title(\"The Fit Normal Distributions\", fontsize=14)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## Plot the discriminant lines\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.plot(np.linspace(X_train.petal_width.min()-1,\n",
    "                        X_train.petal_length.max()+1,100),\n",
    "        delta_c(np.linspace(X_train.petal_width.min()-1,\n",
    "                        X_train.petal_length.max()+1,100), mu_0_hat, sigma_hat_sq, 1/3),\n",
    "            label=\"delta_0_hat\")\n",
    "\n",
    "plt.plot(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100),\n",
    "        delta_c(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100), mu_1_hat, sigma_hat_sq, 1/3),\n",
    "            '--',\n",
    "            label=\"delta_1_hat\")\n",
    "\n",
    "\n",
    "plt.plot(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100),\n",
    "        delta_c(np.linspace(X_train.petal_length.min()-1,\n",
    "                        X_train.petal_length.max()+1,100), mu_2_hat, sigma_hat_sq, 1/3),\n",
    "            '-.',\n",
    "            label=\"delta_2_hat\")\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.title(\"The Estimated Descriminant Lines\", fontsize=14)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Plot the model predictions\n",
    "x = np.linspace(0.01, X_train.petal_length.max()+1,100)\n",
    "y = LDA.predict(x.reshape(-1,1))\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(x[y==0], y[y==0], c='blue')\n",
    "plt.scatter(x[y==1], y[y==1], c='orange')\n",
    "plt.scatter(x[y==2], y[y==2], c='green')\n",
    "\n",
    "plt.xlabel(\"Petal Length (cm)\", fontsize=12)\n",
    "plt.ylabel(\"Predicted Iris Class\", fontsize=12)\n",
    "plt.title(\"Model Predictions\", fontsize=14)\n",
    "plt.yticks([0,1,2], fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.xlim(0,7)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa965c1",
   "metadata": {},
   "source": [
    "### Multiple features, $p>1$\n",
    "\n",
    "We assume that the distribution of the features for each class is a probability density function for a multivariate normal distribution with a class specific mean vector and a common covariance matrix. This is denoted as $(X|y=c)\\sim N(\\mu_c, \\Sigma)$, where $\\mu_c = E(X|y=c)$ and $\\text{cov}(X) = \\Sigma$.\n",
    "\n",
    "A bivariate normal is shown in this image:\n",
    "\n",
    "<img src=\"lecture_9_assets/Multivariate_Gaussian.png\" width=\"60%\"></img>\n",
    "\n",
    "This particular distribution has \n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\n",
    "1 \\\\ 2\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\textrm{ and }\n",
    "\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "0.3 & 0.1\\\\ 0.1 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The eigevectors of the matrix correspond to the principle axes of the level sets of the pdf.\n",
    "\n",
    "You can find the code to generate this image, and further explanation of multivariate normal distributions, in the week 8 math hour notebook.\n",
    "\n",
    "In this case $f_c(X)$ is given as follows:\n",
    "\n",
    "$$\n",
    "f_c(X) = \\frac{1}{(2\\pi)^{m/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2}(X-\\mu_c)^T \\Sigma^{-1} (X-\\mu_c) \\right),\n",
    "$$\n",
    "\n",
    "which will result in a class specific discriminant function of:\n",
    "\n",
    "$$\n",
    "\\delta_c(X) = X^T \\Sigma^{-1} \\mu_c - \\frac{1}{2}\\mu_c^T\\Sigma^{-1} \\mu_c + \\log(\\pi_c).\n",
    "$$\n",
    "\n",
    "The LDA classifier will select the class, $c$, with highest estimated $\\delta_c(X)$.\n",
    "\n",
    "Estimation of the $\\mu_k$ and $\\Sigma$ are similar to the single feature case.\n",
    "\n",
    "Implementation in `sklearn` is identical to the single feature case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8885d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a new model object\n",
    "LDA = LinearDiscriminantAnalysis()\n",
    "\n",
    "## Fit that model\n",
    "LDA.fit(X_train[['petal_width', 'petal_length']].values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a grid\n",
    "p_width_min, p_width_max = X_train.petal_width.min()-.1, X_train.petal_width.max()+.1\n",
    "p_length_min, p_length_max = X_train.petal_length.min()-.1, X_train.petal_length.max()+.1\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(p_width_min, p_width_max, .01),\n",
    "                          np.arange(p_length_min, p_length_max, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "preds = LDA.predict(X_pred)\n",
    "\n",
    "\n",
    "\n",
    "## plotting the decision boundary with the training points\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(X_pred[preds==0,0],\n",
    "            X_pred[preds==0,1],\n",
    "            alpha=.01)\n",
    "plt.scatter(X_pred[preds==1,0],\n",
    "            X_pred[preds==1,1],\n",
    "            alpha=.01)\n",
    "plt.scatter(X_pred[preds==2,0],\n",
    "            X_pred[preds==2,1],\n",
    "            alpha=.01,\n",
    "            color='lightgreen')\n",
    "\n",
    "plt.scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "plt.scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker='v',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "plt.scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=100)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "plt.ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "plt.legend(fontsize=12, loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e5bcc",
   "metadata": {},
   "source": [
    "## Quadratic discriminant analysis (QDA)\n",
    "\n",
    "In <i>quadratic discriminant analysis</i> (QDA) we relax the assumption that the covariance matrix is the same across all classes. We thus have that $X|y=c \\sim N(\\mu_c, \\Sigma_c)$, where $\\Sigma_c$ is the covariance matrix of $X|y=c$.\n",
    "\n",
    "When we perform QDA we assign to $X^*$ the class for which:\n",
    "\n",
    "$$\n",
    "\\delta_c(X^*)  = -\\frac{1}{2} \\left( X^* - \\mu_c \\right)^T \\Sigma_c^{-1}  \\left(X^* - \\mu_c  \\right) - \\frac{1}{2}\\log\\left(|\\Sigma_c| \\right) + \\log(\\pi_c) \n",
    "$$\n",
    "\n",
    "$$\n",
    "= -\\frac{1}{2} X^{*T} \\sigma^{-1}_c X^* + X^{*T} \\sigma^{-1}_c \\mu_c - \\frac{1}{2} \\mu_c^T \\sigma_c^{-1} \\mu_c - \\frac{1}{2}\\log\\left(|\\Sigma_c| \\right) + \\log(\\pi_c)\n",
    "$$\n",
    "\n",
    "Let's demonstrate the difference using our `iris` data set and `sklearn`.\n",
    "\n",
    "QDA is implemented in `sklearn` with `QuadraticDiscriminantAnalysis`, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\">https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d757c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing QDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a QDA object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f641bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit QDA object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c97764",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making a grid\n",
    "p_width_min, p_width_max = X_train.petal_width.min()-.1, X_train.petal_width.max()+.1\n",
    "p_length_min, p_length_max = X_train.petal_length.min()-.1, X_train.petal_length.max()+.1\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(p_width_min, p_width_max, .01),\n",
    "                          np.arange(p_length_min, p_length_max, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "LDA_preds = LDA.predict(X_pred)\n",
    "QDA_preds = QDA.predict(X_pred)\n",
    "\n",
    "\n",
    "\n",
    "## plotting the decision boundary with the training points\n",
    "fig,ax = plt.subplots(1,2, figsize=(12,6))\n",
    "\n",
    "\n",
    "## LDA first\n",
    "### Decision Boundaries\n",
    "ax[0].scatter(X_pred[LDA_preds==0,0],\n",
    "            X_pred[LDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==1,0],\n",
    "            X_pred[LDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==2,0],\n",
    "            X_pred[LDA_preds==2,1],\n",
    "            c='lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training Points\n",
    "ax[0].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[0].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                marker='v',\n",
    "                c = 'darkorange',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[0].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                marker='x',\n",
    "                c = 'darkgreen',\n",
    "                s=100)\n",
    "ax[0].set_title(\"LDA Decision Boundary\", fontsize=14)\n",
    "ax[0].set_xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "ax[0].set_ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "## QDA second\n",
    "### Decision Boundaries\n",
    "ax[1].scatter(X_pred[QDA_preds==0,0],\n",
    "            X_pred[QDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==1,0],\n",
    "            X_pred[QDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==2,0],\n",
    "            X_pred[QDA_preds==2,1],\n",
    "            c='lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training Points\n",
    "ax[1].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[1].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker='v',\n",
    "                edgecolor='black',\n",
    "                s=100)\n",
    "ax[1].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker='x',\n",
    "                s=100)\n",
    "\n",
    "ax[1].set_title(\"QDA Decision Boundary\", fontsize=14)\n",
    "ax[1].set_xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "ax[1].set_ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "ax[0].legend(fontsize=12, loc=2)\n",
    "ax[1].legend(fontsize=12, loc=2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d7c9b",
   "metadata": {},
   "source": [
    "As we can see a key difference between LDA and QDA as classifiers is the shape of the decision boundary. \n",
    "\n",
    "- The LDA decision boundaries are hyperplanes \n",
    "    - In 2-D this means that our feature plane is split using lines, in 3-D planes, in higher dimensions hyperplanes. \n",
    "- The QDA decision boundaries are quadratic surfaces, such as ellipsoids, hyperboloids, etc.\n",
    "\n",
    "Recalling our bias-variance tradeoff, LDA has more bias, while QDA has more variance. The one that you want depends on your data set. In this example I think the LDA model would better generalize to additional iris observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5712eb",
   "metadata": {},
   "source": [
    "## Na&#xEF;ve Bayes classifier\n",
    "\n",
    "Recall that we are using Bayes' rule to estimate $P(y=c|X)$:\n",
    "\n",
    "$$\n",
    "P(y=c|X=X^*) = \\frac{\\pi_c f_c(X^*)}{\\sum\\limits_{l=1}^\\mathcal{C} \\pi_l f_l(X^*)}.\n",
    "$$\n",
    "\n",
    "In order to make this estimate we have to:\n",
    "- estimate $\\mathcal{C}$ $\\pi_c$:\n",
    "    - this is straightforward\n",
    "- estimate $\\mathcal{C}$ $m$-dimensional density functions, $f_c(X)$\n",
    "    - not so straightforward\n",
    "    \n",
    "In LDA and QDA we make strong assumptions regarding the form of the density functions. This allows us to take a hard estimation problem, and turn it into a much easier estimation problem. However, these are strong assumptions that could be way off.\n",
    "\n",
    "### Assuming independence\n",
    "\n",
    "The na&#xEF;ve Bayes classifier takes a different approach. Instead of assuming a set form for the density we instead assume that within a given class, $c$, each of $m$ features are independent. That allows us to write:\n",
    "\n",
    "$$\n",
    "f_c(X) = f_{c_1}(X_1) \\times f_{c_2}(X_2) \\times \\dots \\times f_{c_m}(X_m),\n",
    "$$\n",
    "\n",
    "where $f_{c_j}(X_j)$ denotes the probability density function for $X_j$ among observations of the $c^\\text{th}$ class.\n",
    "\n",
    "Under this assumption we have:\n",
    "\n",
    "$$\n",
    "P(y=c|X=X^*) = \\frac{\\pi_c f_{c_1}(X_1^*) \\times f_{c_2}(X_2^*) \\times \\dots \\times f_{c_m}(X_m^*)}{\\sum\\limits_{l=1}^\\mathcal{C} \\pi_l f_{l_1}(X_1^*) \\times f_{l_2}(X_2^*) \\times \\dots \\times f_{l_m}(X_m^*)}.\n",
    "$$\n",
    "\n",
    "Assuming independence between the feature variables allows us to turn our difficult problem of estimating an $m$-dimensional probability distribution (which involves estimating both $m$ marginal distributions and a joint distribution) into a problem where we just have to estimate $m$ independent univariate probability distributions, which is much more tractable.\n",
    "\n",
    "### Estimating the $f_{c_j}$\n",
    "\n",
    "When it comes to estimating the $f_{c_j}$ we typically assume some kind of distribution and then estimate the parameters for that distribution accordingly.\n",
    "\n",
    "For example:\n",
    "- if $X_j$ is quantitative we typically assume it is a normal distribution\n",
    "    - note this is different than LDA or QDA because those do not assume independence, hence the covariance matrices $\\Sigma$ or $\\Sigma_c$.\n",
    "    - phrased different you could say that Naive Bayes with normal distributions is a type of QDA where we further assume that the $\\Sigma_c$ are all diagonal matrices.\n",
    "- if $X_j$ is categorical we could just use a Bernouli distribution (think biased coin toss) estimating the value of $p$ using the proportion of observations where $y=c$ for each possible value of $X_j$.\n",
    "\n",
    "### Is independence a good assumption?\n",
    "\n",
    "Assuming independence is not always a <i>good</i> assumption in the sense that the some of the features may in fact be related to one another. However, even if this assumption does not hold, we can still get decent to good classifiers using na&#xEF;ve Bayes. This can be particularly true if we do not have enough data to reasonably estimate a joint probability distribution. We can think of this assumption as adding bias to our model.\n",
    "\n",
    "Note that a probabilistic classifier can perform well even if it doesn't actually model the probability very well!  If all we care about is classification, it is really the decision boundaries which matter.  In sklearn parlance, `.predict` can still perform well even if `.predict_proba` is way off.\n",
    "\n",
    "Let's show how to implement this algorithm in `sklearn` with the `iris` data set.\n",
    "\n",
    "Na&#xEF;ve Bayes is implemented in `sklearn` with a few different methods, all found in the `naive_bayes` module, <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes\">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes</a>.\n",
    "\n",
    "Because the two features we will use are quantitative we will use the `GaussianNB` model, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\">https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f53537",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the model\n",
    "nb = \n",
    "\n",
    "## Fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 75\n",
    "\n",
    "## Making a grid\n",
    "p_width_min, p_width_max = X_train.petal_width.min()-.1, X_train.petal_width.max()+.1\n",
    "p_length_min, p_length_max = X_train.petal_length.min()-.1, X_train.petal_length.max()+.1\n",
    "\n",
    "xx1, xx2 = np.meshgrid(np.arange(p_width_min, p_width_max, .01),\n",
    "                          np.arange(p_length_min, p_length_max, .01))\n",
    "\n",
    "X_pred = np.zeros((len(xx1.reshape(-1,1)), 2))\n",
    "X_pred[:,0] = xx1.flatten()\n",
    "X_pred[:,1] = xx2.flatten()\n",
    "\n",
    "LDA_preds = LDA.predict(X_pred)\n",
    "QDA_preds = QDA.predict(X_pred)\n",
    "NB_preds = nb.predict(X_pred)\n",
    "\n",
    "\n",
    "\n",
    "## plotting the decision boundary with the training points\n",
    "\n",
    "## make the subplots\n",
    "fig,ax = plt.subplots(1,3,figsize=(16,5))\n",
    "\n",
    "## Plot the LDA first\n",
    "### Decision Boundaries\n",
    "ax[0].scatter(X_pred[LDA_preds==0,0],\n",
    "            X_pred[LDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==1,0],\n",
    "            X_pred[LDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[0].scatter(X_pred[LDA_preds==2,0],\n",
    "            X_pred[LDA_preds==2,1],\n",
    "            c = 'lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training Data\n",
    "ax[0].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=s)\n",
    "ax[0].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker = 'v',\n",
    "                edgecolor='black',\n",
    "                s=s)\n",
    "ax[0].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=s)\n",
    "ax[0].set_title(\"LDA Decision Boundary\", fontsize=14)\n",
    "ax[0].set_xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "ax[0].set_ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "\n",
    "## Plot the QDA next\n",
    "### Decision boundaries\n",
    "ax[1].scatter(X_pred[QDA_preds==0,0],\n",
    "            X_pred[QDA_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==1,0],\n",
    "            X_pred[QDA_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[1].scatter(X_pred[QDA_preds==2,0],\n",
    "            X_pred[QDA_preds==2,1],\n",
    "            c = 'lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### The training data\n",
    "ax[1].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=s)\n",
    "ax[1].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker = 'v',\n",
    "                edgecolor='black',\n",
    "                s=s)\n",
    "ax[1].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=s)\n",
    "\n",
    "ax[1].set_title(\"QDA Decision Boundary\", fontsize=14)\n",
    "ax[1].set_xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "ax[1].set_ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "\n",
    "\n",
    "## Plot the naive bayes finally\n",
    "### Decision Boundaries\n",
    "ax[2].scatter(X_pred[NB_preds==0,0],\n",
    "            X_pred[NB_preds==0,1],\n",
    "            alpha=.01)\n",
    "ax[2].scatter(X_pred[NB_preds==1,0],\n",
    "            X_pred[NB_preds==1,1],\n",
    "            alpha=.01)\n",
    "ax[2].scatter(X_pred[NB_preds==2,0],\n",
    "            X_pred[NB_preds==2,1],\n",
    "            c = 'lightgreen',\n",
    "            alpha=.01)\n",
    "\n",
    "### Training data\n",
    "ax[2].scatter(X_train.loc[y_train==0].petal_width, \n",
    "                X_train.loc[y_train==0].petal_length,\n",
    "                label='Training 0',\n",
    "                c = 'blue',\n",
    "                edgecolor='black',\n",
    "                s=s)\n",
    "ax[2].scatter(X_train.loc[y_train==1].petal_width, \n",
    "                X_train.loc[y_train==1].petal_length,\n",
    "                label='Training 1',\n",
    "                c = 'darkorange',\n",
    "                marker = 'v',\n",
    "                edgecolor='black',\n",
    "                s=s)\n",
    "ax[2].scatter(X_train.loc[y_train==2].petal_width, \n",
    "                X_train.loc[y_train==2].petal_length,\n",
    "                label='Training 2',\n",
    "                c = 'darkgreen',\n",
    "                marker = 'x',\n",
    "                s=s)\n",
    "\n",
    "ax[2].set_title(\"NB Decision Boundary\", fontsize=14)\n",
    "ax[2].set_xlabel(\"Petal Width (cm)\", fontsize=12)\n",
    "ax[2].set_ylabel(\"Petal Length (cm)\", fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax[0].legend(fontsize=10, loc=2)\n",
    "ax[1].legend(fontsize=10, loc=2)\n",
    "ax[2].legend(fontsize=10, loc=2)\n",
    "\n",
    "plt.subplots_adjust(wspace=.1)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean of each class.  These should be identical.\n",
    "\n",
    "LDA.means_, QDA.means_, nb.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LDA common covariance matrix \\n\")\n",
    "print(LDA.covariance_, '\\n')\n",
    "\n",
    "print(\"QDA covariance matrix for each class \\n\")\n",
    "print(\"Class 0\")\n",
    "print(QDA.covariance_[0], '\\n')\n",
    "print(\"Class 1\")\n",
    "print(QDA.covariance_[1], '\\n')\n",
    "print(\"Class 2\")\n",
    "print(QDA.covariance_[2], '\\n')\n",
    "\n",
    "print(\"NB variance for feature and each class \\n\")\n",
    "print(nb.var_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292c6d7d",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erdős Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
