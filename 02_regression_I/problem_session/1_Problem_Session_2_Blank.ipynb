{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ad11a2",
   "metadata": {},
   "source": [
    "# Problem Session 2\n",
    "\n",
    "In this problem session we practice our skills with :\n",
    "\n",
    "* Exploratory Data Analysis\n",
    "* Simple linear regression\n",
    "* Multiple linear regression\n",
    "* k nearest neighbors regression\n",
    "* kFold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ca66c4",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## We first load in packages we will need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa5526",
   "metadata": {},
   "source": [
    "#### 1. Introducing the data\n",
    "\n",
    "Our data concerns Median house prices for California districts derived from the 1990 census.\n",
    "\n",
    "This dataset was found on Kaggle.com, <a href=\"https://www.kaggle.com/datasets/camnugent/california-housing-prices/data\">https://www.kaggle.com/datasets/camnugent/california-housing-prices/data</a>.\n",
    "\n",
    "##### a. \n",
    "\n",
    "First load the data for this problem. It is stored in the file `housing.csv` in the `data` folder of the repository. After loading the data look at the first five rows of the dataset. Then run `housing.info()`.  Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dcc27d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "housing = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143b82f",
   "metadata": {},
   "source": [
    "Yes, `total_bedrooms` has some missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9202e81",
   "metadata": {},
   "source": [
    "##### b. \n",
    "\n",
    "There are future lecture notebooks that cover ways to <i>impute</i> missing values, but for this notebook you will simply remove the missing values. \n",
    "\n",
    "Use `dropna`, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html</a> to get a version of the data set that has had the missing values removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399467a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45414a96",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "The column `median_house_value` currently contains strings instead of a floats.  Before doing any modeling you will have to clean the data a little bit.\n",
    "\n",
    "Write a function `clean_column` which passes the indicated tests. \n",
    "\n",
    "Then use `.apply`, <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html</a> to apply clean_column to `median_house_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8c228",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define your function below\n",
    "\n",
    "assert clean_column('$432,425.0') == 432425.0\n",
    "assert clean_column('$15,326.0') == 15326.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function to clean the median_house_value column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551611c8",
   "metadata": {},
   "source": [
    "### Predictive Model\n",
    "\n",
    "In the next couple of problem session notebooks you will build a series of models to predict the sale price of a given vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8513e81",
   "metadata": {},
   "source": [
    "#### 2. Train test split\n",
    "\n",
    "The first step in predictive modeling is performing a train test split. Perform a train test split on these data, setting aside $20\\%$ of the data as a test set. Choose a `random_state` so your results are reproducible.\n",
    "\n",
    "As a refresher you can use `sklearn`'s `train_test_split` function: \n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cc22d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "519644af",
   "metadata": {},
   "source": [
    "#### 3. Exploratory data analysis (EDA)\n",
    "\n",
    "After the train test split we can work on some exploratory data analysis. Here is where we start to look at the data and see if we can generate any modeling ideas or hypotheses. You will make a series of plots and learn a modeling trick that should improve any models we make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e0472",
   "metadata": {},
   "source": [
    "##### a. \n",
    "\n",
    "Use `seaborn`'s `pairplot`, <a href=\"https://seaborn.pydata.org/generated/seaborn.pairplot.html\">https://seaborn.pydata.org/generated/seaborn.pairplot.html</a> to plot `median_selling_value` against `km_driven`, `mileage` and `age`. Shell code is provided for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e74aac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a83fa5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# for your convenience I have copied the feature names here.\n",
    "# you could instead get them programmatically by slicing the housing.columns array\n",
    "\n",
    "features = ['median_house_value', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population','households', 'median_income']\n",
    "\n",
    "sns.pairplot(housing_train,\n",
    "                y_vars = ,\n",
    "                x_vars = ,\n",
    "                height = 5,\n",
    "                diag_kind = None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074cc98",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "Do any of the previous relationships look linear? Notice anything else interesting about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d7a6f",
   "metadata": {},
   "source": [
    "The relationship with median_income does appear roughly linear.  Hard to tell with some of the other variables.\n",
    "\n",
    "I notice that `median_house_value` seems to have been truncated at $\\$500000$.  This is a real problem for linear regression!  It will severely bias our estimates.\n",
    "\n",
    "The easiest way to deal with this is to discard all of these rows.\n",
    "\n",
    "A more complicated way would be to try and utilize those rows using something like a [Tobit Model](https://en.wikipedia.org/wiki/Tobit_model).\n",
    "\n",
    "Let's take the easy way out for now.  This gives us another independent test of our model:  after training our model on the rest of the data we can see whether it predicts that those rows have a median value above $\\$500000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set aside all of the rows for which the median house value is 500000\n",
    "housing_truncated = \n",
    "housing_train_truncated = \n",
    "housing_test_truncated = \n",
    "\n",
    "# Redefine these to only include the non-truncated examples\n",
    "housing = \n",
    "housing_train = \n",
    "housing_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8ab7d",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "Another part of EDA is calculating descriptive statistics.\n",
    "\n",
    "One statistic of interest to us in this situation is the <i>Pearson correlation coefficient</i>. For two variables $x$ and $y$ with $n$ observations each, the Pearson correlation is given by:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^n \\left( x_i - \\overline{x} \\right) \\left( y_i - \\overline{y}  \\right)}{\\sqrt{\\sum_{i=1}^n \\left(x_i - \\overline{x}\\right)^2 \\sum_{i=1}^n \\left(y_i - \\overline{y} \\right)^2}} = \\frac{\\text{cov}\\left(x, y\\right)}{\\sigma_x \\sigma_y},\n",
    "$$\n",
    "\n",
    "where $x_i$ is the $i^\\text{th}$ observation, $\\overline{x} = \\sum_{i=1}^n x_i/n$, $\\text{cov}\\left( x, y \\right)$ is the covariance between $x$ and $y$, and $\\sigma_x$ denotes the standard deviation of $x$.\n",
    "\n",
    "$r \\in [-1,1]$ gives a sense of the strength of the linear relationship between $x$ and $y$. The closer $|r|$ is to $1$, the stronger the linear relationship between $x$ and $y$, the sign of $r$ determines the direction of the relationship, with $r < 0$ meaning a line with a negative slope and $r > 0$ a line with a positive slope.\n",
    "\n",
    "Calculate the correlation between `median_house_value` and the columns you have previously plotted.\n",
    "\n",
    "<i>Hint: Either <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html</a> or <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html\">https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html</a> should work.</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15cd18",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba1dfe82",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "Based on your EDA, which feature do you think would best predict `median_house_value` in a simple linear regression model?\n",
    "\n",
    "WARNING:  while using feature/outcome correlation is a reasonable choice for feature selection in a simple linear regression model, it is **not** a good choice for multiple linear regression.  [This stats.stackexchange post](https://stats.stackexchange.com/a/139031/97124) explains why!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad60f8",
   "metadata": {},
   "source": [
    "##### f.\n",
    "\n",
    "We have not yet investigated *spatial* variation in the housing prices.\n",
    "\n",
    "Use [https://plotly.com/python/mapbox-density-heatmaps/](https://plotly.com/python/mapbox-density-heatmaps/) as inspiration and make a heatmap of `median_house_value`.\n",
    "\n",
    "Does it seem like including the latitude and longitude somehow in our model would be helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecdf585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e915b5cc",
   "metadata": {},
   "source": [
    "#### 4. Modeling\n",
    "\n",
    "Now you will build some preliminary models for this data set.\n",
    "\n",
    "##### a.\n",
    "\n",
    "When doing predictive modeling it is good practice to have a <i>baseline model</i> which is a simple \"model\" solely for comparison purposes. These are not, typically, complex or good models, but they are important reference points to give us a sense of how well our models are actually performing.\n",
    "\n",
    "A standard regression model baseline is to just predict the average value of $Y$ for any value of $X$. In this setting that model looks like this:\n",
    "\n",
    "$$\n",
    "\\text{Baseline Model: } \\ \\ \\ \\ \\text{Median House Value} = \\mathbb{E}\\left(\\text{Median House Value}\\right) + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is i.i.d. and normally distributed.\n",
    "\n",
    "Write some code to estimate $\\mathbb{E}\\left(\\text{Median House Value}\\right)$ using the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf710f5",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "992d3595",
   "metadata": {},
   "source": [
    "Below you will use cross-validation to compare one simple linear regression models, one multiple linear regression model, and one kNN model which uses the spatial data.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Baseline Model}:& \\ \\text{Median House Value} = \\mathbb{E}\\left(\\text{Median House Value}\\right) + \\epsilon\\\\\n",
    "\n",
    "\\text{SLR Model}:& \\ \\text{Median House Value} = \\beta_0 + \\beta_1 \\left( \\text{Median Income} \\right) + \\epsilon\\\\\n",
    "\n",
    "\\text{MLR model}:& \\ \\text{Median House Value} = \\beta_0 + \\beta_1 \\left(\\text{Median Income}\\right)  + \\beta_2 \\left(\\text{Households}\\right) + \\epsilon\\\\\n",
    "\n",
    "\\text{kNN model}:& \\ \\text{Use k nearest neighbors regression on latitude and longitude with $k = 10$}\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We will attempt hyperparameter tuning on $k$ in a later problem session, but just stick with $k=10$ for now.\n",
    "\n",
    "##### b.\n",
    "In this problem practice fitting just the MLR model using the training set and `sklearn`'s `LinearRegression` model, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55655cfb",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import the linear regression model\n",
    "\n",
    "# instantiate a model object\n",
    "mlr_model =\n",
    "\n",
    "# Fit the model to the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287da8c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "mlr_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4cae6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "mlr_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be357e",
   "metadata": {},
   "source": [
    "##### c.\n",
    "\n",
    "In this problem you will try to implement $5$-fold cross-validation (CV) to compare these three models and the baseline model to see which one has the lowest average cross-validation root mean squared error (RMSE).\n",
    "\n",
    "Because this may be your first time implementing CV, some of the code will be filled in for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b641ed",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## import KFold and kNeighborsRegressor here.\n",
    "\n",
    "## import root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cdd31",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Make a KFold object\n",
    "## remember to set a random_state and set shuffle = True\n",
    "num_splits = 5\n",
    "num_models = 4\n",
    "kfold = \n",
    "\n",
    "## This array will hold the mse for each model and split\n",
    "rmses = np.zeros((num_models, num_splits))\n",
    "\n",
    "## sets a split counter\n",
    "i = 0\n",
    "\n",
    "## loop through the kfold here\n",
    "for train_index, test_index in     :\n",
    "    ## cv training set\n",
    "    housing_tt = \n",
    "    \n",
    "    ## cv holdout set\n",
    "    housing_ho = \n",
    "    \n",
    "    ## \"Fit\" and get ho rmse for the baseline model.\n",
    "    ## No need to use an sklearn function:  just get the mean.  \n",
    "    ## baseline_pred should be a numpy array with the same number of elements as housing_ho\n",
    "\n",
    "    baseline_pred = \n",
    "    \n",
    "    rmses[0, i] = \n",
    "    \n",
    "    ## Fit and get ho rmse for slr model\n",
    "    slr = \n",
    "    \n",
    "    \n",
    "    ## Fit and get ho mse for mlr model\n",
    "    mlr = \n",
    "    \n",
    "\n",
    "    ## Fit and get ho rmse for the spatial model\n",
    "    knn = \n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad1ee3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "## Find the avg cv mse for each model here\n",
    "print(f\"Baseline Avg. CV RMSE: {np.mean(rmses[0,:])} and STD: {np.std(rmses[0,:])}\")\n",
    "print(f\"SLR Avg. CV MSE: {np.mean(rmses[1,:])} and STD: {np.std(rmses[1,:])}\")\n",
    "print(f\"MLR Avg. CV MSE: {np.mean(rmses[2,:])} and STD: {np.std(rmses[2,:])}\")\n",
    "print(f\"Spatial Avg. CV MSE: {np.mean(rmses[3,:])} and STD: {np.std(rmses[3,:])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6cddc",
   "metadata": {},
   "source": [
    "##### d.\n",
    "\n",
    "Which model had the lowest average cross validation root mean squared error?  \n",
    "\n",
    "Discuss the meaning of the STD in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22753078",
   "metadata": {},
   "source": [
    "##### e.\n",
    "\n",
    "Train the simple linear regression model on the full training set and predict on the truncated dataset.  Does the model predict that the median house values are in excess of $\\$500000$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0f561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02558a02",
   "metadata": {},
   "source": [
    "That's it for this notebook. In the next couple of regression based notebooks we will build additional models for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4d8e5",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erdős Institute Data Science Boot Camp by Steven Gubkin.\n",
    "\n",
    "Please refer to the license in this repo for information on redistribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
