{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8d37e59",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook will be some additional problems regarding logistic regression. This material corresponds to `Lectures/Supervised Learning/Classification/4. Logistic Regression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af7c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a05414",
   "metadata": {},
   "source": [
    "##### 1. How to Fit a Logistic Regression Model\n",
    "\n",
    "We return to Maximum Likelihood Estimation from the our Regression `Practice Problems`.\n",
    "\n",
    "Recall that in logistic regression we are interested in $P(y=1|X)$ let's call this $p(X;\\beta)$. In logistic regression we are modeling this as:\n",
    "$$\n",
    "p(X;\\beta) = \\frac{1}{1 + e^{-X\\beta}}.\n",
    "$$\n",
    "\n",
    "Now because our training data exists in a binary state we cannot rely on the same procedure we did for linear regression. We instead use maximum likelihood estimation. We first must write out the likelihood function.\n",
    "\n",
    "First attempt to set up the $\\log$-likelihood for the logistic regression model, hint: we can think of $y_i$ as a bernouli random variable with probability parameter $p_i=p(X_i;\\beta)$.\n",
    "\n",
    "\n",
    "After you have accomplished that read through this reference starting at page 5 to see the derivation of the maximum likelihood estimate for logistic regression, <a href=\"https://cseweb.ucsd.edu/~elkan/250B/logreg.pdf\">https://cseweb.ucsd.edu/~elkan/250B/logreg.pdf</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817cd78",
   "metadata": {},
   "source": [
    "##### Sample Solution\n",
    "\n",
    "The likelihood function is:\n",
    "$$\n",
    "\\prod_{i=1}^n P(y=y_i|X_i) = \\prod_{i=1}^n p(X_i;\\beta)^{y_i} (1 - p(X_i;\\beta))^{1-y_i}.\n",
    "$$\n",
    "\n",
    "Thus the log-likelihood is:\n",
    "$$\n",
    "\\sum_{i=1}^n \\log(P(y=y_i|X_i) = \\sum_{i=1}^n y_i \\log(p(X_i;\\beta)) + (1 - y_i)\\log(1 - p(X_i;\\beta)) \n",
    "$$\n",
    "$$\n",
    "= \\sum_{i=1}^n y_i \\log \\left( \\frac{p(X_i;\\beta))}{1 - p(X_i;\\beta)} \\right) + \\log(1 - p(X_i;\\beta)) = \\sum_{i=1}^n y_i X_i \\beta - \\log(1+e^{-X_i\\beta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc84b3e2",
   "metadata": {},
   "source": [
    "##### 2. Regularization for logistic regression\n",
    "\n",
    "You can also implement regularization with logistic regression (ridge, lasso or elastic net). In fact, by default `sklearn`'s `LogisticRegression` is ridge logistic regression.\n",
    "\n",
    "In order to deliberately perform regularized logistic regression you will need/want to know these arguments:\n",
    "- `penalty` this determines what kind of regularization you run:\n",
    "    - `penalty = 'none'` performs normal logistic regression,\n",
    "    - `penalty = 'l2'` performs ridge logistic regression,\n",
    "    - `penalty = 'l1'` performs lasso logistic regression, and\n",
    "    - `penalty = 'elasticnet'` performs elastic net logistic regression.\n",
    "- `C` this controls the strength of the regularization, think of this like the `alpha` argument from `Ridge` and `Lasso`:\n",
    "    - Large `C` results in a weaker regularization (think of this as a small value of `alpha`), and \n",
    "    - Small `C` results in a stronger regularization (equivalent to a large value of `alpha`).\n",
    "- `solver` this is the algorithm that `sklearn` implements to fit the model, check the documentation, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a>, to see what argument you should use for the regularization you want to perform.\n",
    "- `l1_ratio`, if you are performing Elastic Net regularization you need to set this, see the Regularization Regression homework notebook.\n",
    "\n",
    "\n",
    "Load in the iris data set below. Then make a new column `virginica` that says whether an observation is of the virginica class. Attempt to perform feature selection using lasso logistic regression.\n",
    "\n",
    "<i>Do not forget to scale the data prior to fitting the regularization model.<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f48ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to get the iris data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "## Load the data\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris['data'],columns = ['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "iris_df['iris_class'] = iris['target']\n",
    "\n",
    "## import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Making the split\n",
    "iris_train, iris_test = train_test_split(iris_df.copy(), \n",
    "                                            random_state=431,\n",
    "                                            shuffle=True,\n",
    "                                            test_size=.2,\n",
    "                                            stratify=iris_df['iris_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2703902",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train['virginica'] = 1*(iris_train['iris_class'] == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6eab7",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dadc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f066cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "\n",
    "iris_train_scale = scale.fit_transform(iris_train[['sepal_length','sepal_width','petal_length','petal_width']])\n",
    "\n",
    "\n",
    "Cs = [100,10,1,.1,.01,.001,.0001,.00001,.000001,.0000001]\n",
    "\n",
    "coefs = np.zeros((len(Cs), 4))\n",
    "\n",
    "i = 0\n",
    "for C in Cs:\n",
    "    log_reg = LogisticRegression(penalty='l1',C=C,solver='liblinear')\n",
    "    log_reg.fit(iris_train_scale, iris_train['virginica'])\n",
    "    \n",
    "    coefs[i,:] = log_reg.coef_\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098ef050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C=100</th>\n",
       "      <td>-8.575021</td>\n",
       "      <td>-7.911570</td>\n",
       "      <td>49.931031</td>\n",
       "      <td>18.482984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=10</th>\n",
       "      <td>-1.987029</td>\n",
       "      <td>-2.022275</td>\n",
       "      <td>13.044011</td>\n",
       "      <td>6.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.635549</td>\n",
       "      <td>3.090743</td>\n",
       "      <td>3.599688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=0.1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.506548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=0.01</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=0.001</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=0.0001</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1e-05</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1e-06</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C=1e-07</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sepal_length  sepal_width  petal_length  petal_width\n",
       "C=100        -8.575021    -7.911570     49.931031    18.482984\n",
       "C=10         -1.987029    -2.022275     13.044011     6.304500\n",
       "C=1           0.000000    -0.635549      3.090743     3.599688\n",
       "C=0.1         0.000000     0.000000      0.000000     1.506548\n",
       "C=0.01        0.000000     0.000000      0.000000     0.000000\n",
       "C=0.001       0.000000     0.000000      0.000000     0.000000\n",
       "C=0.0001      0.000000     0.000000      0.000000     0.000000\n",
       "C=1e-05       0.000000     0.000000      0.000000     0.000000\n",
       "C=1e-06       0.000000     0.000000      0.000000     0.000000\n",
       "C=1e-07       0.000000     0.000000      0.000000     0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(coefs, \n",
    "             index=[\"C=\"+str(C) for C in Cs], \n",
    "             columns = ['sepal_length','sepal_width','petal_length','petal_width']).round(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952ce68",
   "metadata": {},
   "source": [
    "It appears that `petal_width` is the most important, followed by `petal_length` then `sepal_width`. The least important seems to be `sepal_length`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa920b2",
   "metadata": {},
   "source": [
    "##### 3. Interpreting coefficients for categorical features\n",
    "\n",
    "While we did not build a logistic regression model using categorical features in our lecture notebook, this can be done. Just like we built $K-1$ dummy variables for a feature with $K$ possible categories, we do the same for logistic regression. So if we wanted to use a feature with $2$ possible categories, we would need a single dummmy variable. If we wanted to use a feature with $5$ possible categories, we would need $4$ dummy variables.\n",
    "\n",
    "While the process for adding a categorical feature to a logistic regression model is the same, the way the coefficient estimate for such a feature is interpreted is slightly different.\n",
    "\n",
    "In order to help explain the interpretation let's fit a model together.\n",
    "\n",
    "First generate the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c10a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(135135)\n",
    "X = np.zeros((200,2))\n",
    "y = np.zeros(200)\n",
    "X[:,0] = 10*np.random.random(200)\n",
    "\n",
    "X[:101,1] = 0\n",
    "X[101:,1] = 1\n",
    "\n",
    "y[X[:,0] > 7] = 1\n",
    "y[:101][(X[:101,0] > 3) & (X[:101,0] <=7)] = np.random.binomial(1, .9, np.sum((X[:101,0] > 3) & (X[:101,0] <=7)))\n",
    "y[101:][(X[101:,0] > 3) & (X[101:,0] <=7)] = np.random.binomial(1, .1, np.sum((X[101:,0] > 3) & (X[101:,0] <=7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a0eea",
   "metadata": {},
   "source": [
    "This problem has two features, one is a continuous feature, $X_1$, the other a binary, $X_2$. Now use `LogisticRegression` to fit the model regressing `y` on `X`. <i>Here we will not need to make dummy variables because $X_2$ is already a binary</i>.\n",
    "\n",
    "$$\n",
    "P(y=1|X) = \\frac{1}{1+e^{-\\left( \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \\right)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83943bee",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e9213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6d31c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0954820",
   "metadata": {},
   "source": [
    "Now look at the coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95a8aede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0201619 , -2.90714949]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfdd8d",
   "metadata": {},
   "source": [
    "Remember how we set up that the logistic regression model was a linear model for the $\\log$-odds that $y=1$,\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p(X)}{1-p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2, \\text{ or } \\text{Odds}|X = C e^{\\beta_1 X_1 + \\beta_2 X_2}.\n",
    "$$\n",
    "\n",
    "We can interpret the coefficient on the binary variable $X_2$ by making a comparison to a baseline case, say $X_2=0$.\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Odds}|X_1 = X_1^*, X_2 = 1}{\\text{Odds}|X_1 = X_1^*, X_2 = 0} = \\frac{C e^{\\beta_1 X_1^* + \\beta_2 (1)}}{C e^{\\beta_1 X_1^* + \\beta_2 (0)}} = e^{\\beta_2},\n",
    "$$\n",
    "\n",
    "and so we can interpret $\\beta_2$ by saying that:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "    The odds that $y=1$ when $X_2=1$ are $e^{\\beta_2}$ times the odds that $y=1$ when $X_2=0$ holding all other variables equal.\n",
    "</center>\n",
    "\n",
    "This interpretation is sometimes called the <i>odds ratio</i> of $X_2 = 1$ to $X_2=0$.\n",
    "\n",
    "Interpret the estimate of $\\beta_2$ for the model we just fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "495b0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odds that y=1 when X2=1 are 0.054631235141658414 times the odds that y=1 when X2=0 holding X1 constant.\n"
     ]
    }
   ],
   "source": [
    "print(\"The odds that y=1 when X2=1 are\",\n",
    "         np.exp(log_reg.coef_[0][1]),\n",
    "         \"times the odds that y=1 when X2=0\",\n",
    "         \"holding X1 constant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000a1cf2",
   "metadata": {},
   "source": [
    "This is the same way you would interpret the $K-1$ coefficients for a feature with $K$ possible classes, where the reference variable is the one for which you did not make a dummy variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e453624",
   "metadata": {},
   "source": [
    "##### 4. Multiclass logistic regression (multinomial regression)\n",
    "\n",
    "While we formulated logistic regression for binary classification, multiclass classification is also possible.\n",
    "\n",
    "Suppose we have $m$ features stored in a variable $X$ that we would like to use to predict a variable $y$ that takes on values $1, 2, \\dots , K$. \n",
    "\n",
    "The multinomial logistic regression model regressing $y$ on $X$ is:\n",
    "\n",
    "$$\n",
    "P(y=k | X=X^*) = \\frac{\\exp(X^*\\beta^{(k)})}{1+\\sum_{l=1}^{K-1} \\exp(X^*\\beta^{(l)}) }, \\text{ for } k = 1,\\dots,K-1, \\text{ and}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y=K | X=X^*) = \\frac{1}{1 + \\sum_{l=1}^{K-1} \\exp(X^*\\beta^{(l)}) },\n",
    "$$\n",
    "\n",
    "where the $\\beta^{(l)}$ are class specific coefficient vectors.\n",
    "\n",
    "This is similar to when we have a categorical input variable, as we can see:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{P(y=k| X=X^*)}{P(y=K| X=X^*)} \\right) = X^* \\beta^{(k)}.\n",
    "$$\n",
    "\n",
    "\n",
    "It is possible to fit the multinomial logistic regression model with `sklearn`'s `LogisticRegression` model. This can be done by setting the `multi_class` argument to `'multinomial'` when creating the `LogisticRegression` model object.\n",
    "\n",
    "Do so to fit a multinomial logistic regression model predicting the iris class. What is the training accuracy of this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f267ecd",
   "metadata": {},
   "source": [
    "##### Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f553194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=500, multi_class='multinomial')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi = LogisticRegression(multi_class='multinomial', max_iter=500)\n",
    "\n",
    "multi.fit(iris_train[['sepal_length','sepal_width','petal_length','petal_width']], iris_train['iris_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f5b0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f157daef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(iris_train['iris_class'], \n",
    "               multi.predict(iris_train[['sepal_length','sepal_width','petal_length','petal_width']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c46e4b",
   "metadata": {},
   "source": [
    "##### 5. Generalized linear models (GLMs)\n",
    "\n",
    "<i>This is not an exercise, just read the following.</i>\n",
    "\n",
    "Let's review the two types of regression models we've discussed.\n",
    "\n",
    "#### Linear Regression\n",
    "\n",
    "For a continuous target, $y$, and a features matrix, $X$, we had:\n",
    "$$\n",
    "E(y|X) = X\\beta.\n",
    "$$\n",
    "\n",
    "#### Logistic Regression\n",
    "\n",
    "For a binary target, $y$, and a feature matrix, $X$, we had:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{P(y=1|X)}{1-P(y=1|X)} \\right) = X\\beta.\n",
    "$$\n",
    "\n",
    "Where we should note that for a binary $0$-$1$ variable $P(y=1|X) = E(y|X)$ so in reality we had:\n",
    "\n",
    "$$\n",
    "\\log\\left( \\frac{E(y|X)}{1-E(y|X)} \\right) = X\\beta.\n",
    "$$\n",
    "\n",
    "#### Notice Anything?\n",
    "\n",
    "In both cases we could write the following:\n",
    "\n",
    "$$\n",
    "g(E(y|X)) = X\\beta,\n",
    "$$\n",
    "\n",
    "where we made a specific choice for the functional form of $g$ depending on the data type of $y$. This is the idea behind generalized linear models.\n",
    "\n",
    "### Three Components\n",
    "\n",
    "Given features, $X$, and target, $y$, a generalized linear model relating $y$ to $X$ is composed of three components. \n",
    "\n",
    "##### I.  Random Component\n",
    "\n",
    "This is where you assume a probability distribution for $y|X$. It is typically assumed that distribution for $y|X$ comes from the <i>exponential family</i>, <a href=\"https://en.wikipedia.org/wiki/Exponential_family\">https://en.wikipedia.org/wiki/Exponential_family</a>.\n",
    "\n",
    "##### II. Systematic Component\n",
    "\n",
    "Where you relate the parameters $\\beta$ to the features $X$. It is always the case in a generalized linear model that the systematic component is $X\\beta$.\n",
    "\n",
    "##### III. Link Component\n",
    "\n",
    "The connection between the random and systematic components.\n",
    "\n",
    "Combining all three of these components gives the following:\n",
    "\n",
    "$$\n",
    "g(E(y|X)) = X\\beta.\n",
    "$$\n",
    "\n",
    "We will not do anything else with generalized linear models in this program or in python. However, as you continue on in your own data science work it may be useful to be familiar with the generalized linear model setup. For those interested in learning more I encourage you to check out the following resources:\n",
    "\n",
    "<a href=\"http://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf\">http://www.stat.cmu.edu/~ryantibs/advmethods/notes/glm.pdf</a>\n",
    "\n",
    "<a href=\"http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf\">http://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe4954",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the Erd≈ës Institute as subject to the license (see License.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f466a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
