{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85bcfebc",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "There are algorithms that fall into a class known as tree-based methods. The foundation for this class is the decision tree.\n",
    "\n",
    "## What we will accomplish\n",
    "\n",
    "In this notebook we will:\n",
    "- Introduce decision trees,\n",
    "- Define Gini Impurity and\n",
    "- Review the CART algorithm used to fit a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e783abf",
   "metadata": {},
   "source": [
    "## The basic idea\n",
    "\n",
    "The basic idea for decision trees is to segment the data space in a way that allows us to classify well.\n",
    "\n",
    "### A simple example\n",
    "\n",
    "Let's look at a simple example. Below we generate some random data with an $x_1$ feature, an $x_2$ feature, and a target, $y$. We then plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(334)\n",
    "X = np.zeros((200,2))\n",
    "X[:,0] = np.random.random(200)\n",
    "X[100:,0] = X[100:,0] + 1.01\n",
    "X[:100,0] = X[:100,0] - .01\n",
    "X[:,1] = np.random.random(200)\n",
    "\n",
    "y = np.zeros(200)\n",
    "y[100:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "plt.scatter(X[y == 0,0], X[y == 0,1], c='blue', label=\"0\")\n",
    "plt.scatter(X[y == 1,0], X[y == 1,1], c='orange', marker=\"v\", label=\"1\")\n",
    "plt.xlabel(\"$x_1$\",fontsize = 12)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 12)\n",
    "plt.legend(fontsize=12, loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1c095",
   "metadata": {},
   "source": [
    "If you had to come up with an adhoc classification rule right now what would you say we should do?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "One good choice would be to say that if the data point is to the left of $x_1= 1$ then we classify it as $0$, and if it to the right of $x_1=1$ then we classify it as $1$. We can illustrate this with a logic tree:\n",
    "<img src = \"lecture_10_assets/SimpleTree.png\" width = \"50%\"></img>\n",
    "\n",
    "While this is a simple example, it is the basic idea behind a decision tree. You look at a feature, choose a cut point that minimizes some measure of wrongness, and keep going until some stopping criterion.\n",
    "\n",
    "Before we move on to real data, let's fit a decision tree to our dumb data and see what classification rule it comes up with. `sklearn` fits a decision tree with the model object, `DecisionTreeClassifier` <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d978b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "## tree will be used to plot the decisiton tree\n",
    "from sklearn import tree\n",
    "\n",
    "## This is the actual out of the box algorithm\n",
    "from sklearn.tree import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca95ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a decision tree object\n",
    "tree_clf =\n",
    "\n",
    "## fit the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125db053",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the fitted tree\n",
    "plt.figure(figsize = (5,5))\n",
    "tree.plot_tree(tree_clf, filled = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512ed0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict([[0.1,2],[1.9,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3574cd51",
   "metadata": {},
   "source": [
    "The plot above is the logic tree built by the decision tree algorithm. To classify a new observation we start at the <i>root node</i> up top. If the observation satisfies the logic statement at the top we go left and are classified as a $0$, else we go right and are classified as $1$. The two <i>children</i> of the root node are known as <i>leaf nodes</i> or <i>terminal nodes</i> because they have no children of their own so we just predict the majority class contained in that node.\n",
    "\n",
    "This is essentially the decision rule we came up with (which is the objectively correct one by the way), so in this example the decision tree did well.\n",
    "\n",
    "\n",
    "If we look in the plot above we notice a number of different stats in each node:\n",
    "\n",
    "- `samples`: the number of samples in each node\n",
    "- `gini`: the gini impurity of the node, more on this in a moment\n",
    "- `value`: the breakdown of the number of samples of each target value in the node, for example the leaf node on the left has $100$ nodes labeled $0$ and $0$ nodes labeled $1$\n",
    "- A decision rule: The rule that is used for the following split, samples that would be evaluated as True for the rule go to the left child, samples that would be evaluated as False go to the right child\n",
    "\n",
    "### How is wrongness measured?\n",
    "\n",
    "There are a couple of ways to measure wrongness, or rather impurity, with decision trees. There are two popular measures that can implemented with out of the box `sklearn`.\n",
    "\n",
    "#### Gini impurity\n",
    "\n",
    "Suppose that there are $N$ target classes.\n",
    "\n",
    "The Gini Impurity for class $i$ of a node estimates the probability that a randomly chosen sample of class $i$ from the node is incorrectly classified as not class $i$. The formula is:\n",
    "$$\n",
    "G_i = p_i(1-p_i),\n",
    "$$\n",
    "\n",
    "where $p_i$ is the proportion of samples in the node of class $i$. The total Gini Impurity is the sum of all these $G_i$:\n",
    "\n",
    "$$\n",
    "I_G = \\sum_{i=1}^N G_i = 1 - \\sum_{i=1}^N p_i^2.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Categorical Cross-Entropy\n",
    "\n",
    "Categorical cross-entropy is an alternative impurity measure you could use when building a decision tree in `sklearn`. \n",
    "\n",
    "We have already discussed this metric in the `Multiclass Classification Metrics` notebook.\n",
    "\n",
    "Again suppose there are $N$ target classes. The contribution made to entropy from class $i$ is:\n",
    "\n",
    "$$\n",
    "H_i = - p_i \\log(p_i),\n",
    "$$\n",
    "\n",
    "where again $p_i$ is the proportion of samples in the node of class $i$. The total entropy of the node is the sum of all the $H_i$:\n",
    "\n",
    "$$\n",
    "I_H = \\sum_{i=1}^N H_i = -\\sum_{i=1}^N p_i \\log(p_i).\n",
    "$$\n",
    "\n",
    "#### Which to use?\n",
    "\n",
    "Note that the second order Taylor expansion of $-p\\log(p)$ about $p = 1$ is $p(1-p)$.  So Gini Impurity can be seen as an approximation of categorical cross-entropy.  In practice using one or the other does not seem to impact model performance.  Computing logarithms adds a bit of computational complexity compared to squaring, so the Gini impurity could lead to a slight training time advantage.  \n",
    "\n",
    "To get some intuition about \"how close\" these are, consider the 2 class case at a node with class fractions $p$ and $1-p$.  The Gini impurity is $2p(1-p)$ and the cross entropy is $-p\\log(p)+(1-p)\\log(1-p)$.\n",
    "\n",
    "Plotting both functions on the interval $[0,1]$ (rescaling each to the value $1$ at $p=0.5$) shows not much difference between the two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6392b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(0.0001,0.9999,1000)\n",
    "plt.plot(xs, 4*xs*(1-xs), label = 'Gini')\n",
    "plt.plot(xs, (-xs*np.log(xs) - (1-xs)*np.log(1-xs))/np.log(2), label = 'Entropy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af024285",
   "metadata": {},
   "source": [
    "## How `sklearn` fits a tree\n",
    "\n",
    "\n",
    "### The CART algorithm\n",
    "\n",
    "`sklearn` uses the <i>Classification and Regression Tree</i> or <i>CART</i> algorithm. \n",
    "\n",
    "Suppose your data set has $n$ observations with $m$ features, and for simplicity only $2$ target classes.\n",
    "\n",
    "The algorithm starts with the root node. It then searches through each feature, $k$, and finds a split point, $t_k$, that produces the purest subsets (weighted by the number of samples in each subset), i.e. it finds a $t_k$ that minimizes:\n",
    "$$\n",
    "J(k,t_k) = \\frac{n_\\text{left}}{n} I_\\text{left} + \\frac{n_\\text{right}}{n} I_\\text{right},\n",
    "$$\n",
    "where left and right refers to being left or right of the split point, $t_k$, and $I$ is the impurity measure you choose to use (the default is Gini). \n",
    "\n",
    "Once it finds the $(k,t_k)$ pair that has smallest $J(k,t_k)$, it splits the data according to that decision split.\n",
    "\n",
    "The algorithm then repeats the entire process on each of the children nodes it just produced. This continues until some stopping condition for example:\n",
    "- reaching a maximum depth, controlled with `max_depth`\n",
    "- reaching a minimum number of samples in each node, controlled with `min_samples_leaf`\n",
    "- reaching a minimum weight to be in a node, controlled with `min_weight_fraction_leaf`\n",
    "- etc., see documentation for further options, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</a>.\n",
    "\n",
    "or until it can no longer reduce the impurity by making a cut.\n",
    "\n",
    "\n",
    "We will end by demonstrating the effect of `max_depth` on a new randomly generated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(334)\n",
    "\n",
    "n_samples = 300\n",
    "X = np.random.uniform(low = -2, high = 2, size=(n_samples,2))\n",
    "\n",
    "y = np.zeros(n_samples)\n",
    "y[((X[:,0]-1)**2 +X[:,1]**2 > 1) & (X[:,0]**2 + X[:,1]**2 < 4)] = 1\n",
    "y[((X[:,0]-1)**2 +X[:,1]**2 < 1)] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca01e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.scatter(X[y == 0,0], X[y == 0,1], c = 'red', marker = 'x', label = \"0\")\n",
    "plt.scatter(X[y == 1,0], X[y == 1,1], c = 'orange', marker = 'v',label = \"1\")\n",
    "plt.scatter(X[y == 2,0], X[y == 2,1], c = 'blue', label = \"2\")\n",
    "plt.gca().add_patch(plt.Circle((1, 0), 1, color = 'k', fill = False))\n",
    "plt.gca().add_patch(plt.Circle((0, 0), 2, color = 'k', fill = False))\n",
    "\n",
    "\n",
    "plt.xlabel(\"$x_1$\",fontsize = 12)\n",
    "plt.ylabel(\"$x_2$\",fontsize = 12)\n",
    "plt.legend(fontsize=12, loc=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_helpers import SkNode, traversable_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e503be8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "s = 50\n",
    "\n",
    "for i in range(1,11):\n",
    "    tree_clf = DecisionTreeClassifier(max_depth  = i, random_state = 216)\n",
    "    \n",
    "    tree_clf.fit(X, y)\n",
    "    \n",
    "    accuracy = accuracy_score(y, tree_clf.predict(X))\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "\n",
    "    nodes = traversable_nodes(tree_clf)\n",
    "\n",
    "    for node in nodes.values():\n",
    "        if node.leaf:\n",
    "            (xmin,ymin),(xmax,ymax) = node.find_constraints()\n",
    "            xmin = np.max([xmin,-2])\n",
    "            xmax = np.min([xmax,2])\n",
    "            ymin = np.max([ymin,-2])\n",
    "            ymax = np.min([ymax,2])\n",
    "            def add_patch(face_color):\n",
    "                plt.gca().add_patch(\n",
    "                    patches.Rectangle(\n",
    "                        (xmin, ymin), xmax-xmin, ymax-ymin, linewidth=2, edgecolor='k', facecolor = face_color, zorder = -2, alpha = 0.4)\n",
    "                    )\n",
    "            if node.pred == 0:\n",
    "                add_patch('red')\n",
    "            elif node.pred == 1:\n",
    "                add_patch('orange')\n",
    "            elif node.pred == 2:\n",
    "                add_patch('blue')\n",
    "\n",
    "    plt.scatter(X[y == 0,0], X[y == 0,1], c = 'red', marker = 'x', label = \"0\")\n",
    "    plt.scatter(X[y == 1,0], X[y == 1,1], edgecolors= 'k', c = 'orange', marker = 'v',label = \"1\",alpha=1)\n",
    "    plt.scatter(X[y == 2,0], X[y == 2,1], edgecolors= 'k', c = 'blue', label = \"2\")\n",
    "    plt.gca().add_patch(plt.Circle((1, 0), 1, color = 'k', fill = False))\n",
    "    plt.gca().add_patch(plt.Circle((0, 0), 2, color = 'k', fill = False))\n",
    "\n",
    "    plt.title(f\"Maximum Depth of {i} \\n Accuracy = {accuracy:.4f}\", fontsize=14)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=12)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae398b",
   "metadata": {},
   "source": [
    "Note that the CART algorithm is deterministic aside from edge cases where two different cuts would make equal reductions in the weighted Gini impurity. The only randomness in the algorithm is deciding which of these edge case cuts to choose.  We set `random_state = 216` for reproducibility and to make sure that the latter trees are refinements of the former."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d60e63",
   "metadata": {},
   "source": [
    "Typically for the \"best\" choice of `max_depth` we would run something like cross-validation.\n",
    "\n",
    "Before we close the jupyter notebook on decision trees let's leave with a few take aways.\n",
    "\n",
    "\n",
    "##### Advantages\n",
    "\n",
    "- Interpret-ability :\n",
    "    - Decision Trees are known as a white box algorithm (as opposed to the black box often used to describe machine learning). This is because you are able to entirely describe how a decision tree predicts a data points target using the logic tree,\n",
    "- Very Fast Predictions and\n",
    "- Very little preprocessing of data prior to training.\n",
    "\n",
    "##### Disadvantages\n",
    "\n",
    "- Greediness:\n",
    "    - The algorithm is greedy meaning it may not create the optimal tree. For example, maybe the best tree involves an initial suboptimal cut, the CART algorithm won't find this tree.\n",
    "- Overfitting: \n",
    "    - Decision trees are very prone to overfitting the data, you can control for this using regularization hyperparameters like max_depth and min_samples_split. It's also a good idea to use cv when you can.\n",
    "- Orthogonal Boundaries:\n",
    "    - Because of the process of determining cut points (remember the $t_k$ from the algorithm?) decision boundaries happen at right angles. This means that classes divided by a non-horizontal or non-vertical line the decision tree will have some capturing the boundary. This can be mitigated a bit with PCA.\n",
    "- Sensitive:\n",
    "    - Trees are very sensitive to the training data. Removing or adding a few points can greatly change the decision boundary produced by the algorithm. One way around this is to use an averaged algorithm, like a random forest. We discuss these in a later notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b895327",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "This notebook was written for the Erd&#337;s Institute C&#337;de Data Science Boot Camp by Matthew Osborne, Ph. D., 2023.  Modified by Steven Gubkin 2024.\n",
    "\n",
    "Any potential redistributors must seek and receive permission from Matthew Tyler Osborne, Ph.D. prior to redistribution. Redistribution of the material contained in this repository is conditional on acknowledgement of Matthew Tyler Osborne, Ph.D.'s original authorship and sponsorship of the ErdÅ‘s Institute as subject to the license (see License.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
